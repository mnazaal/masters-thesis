#+LATEX_HEADER: \input{templates/tufte-book}
#+LATEX_CLASS: tufte-book
#+LATEX_COMPILER: pdflatex
#+OPTIONS: toc:nil
# #+OPTIONS: num:1
#+LATEX: \setlength\parindent{0pt}
#+LATEX: \setcounter{secnumdepth}{2}
#+LATEX: \newcommand{\indep}{\perp \!\!\! \perp}


# Plain HTML
# #+HTML_HEAD: <link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
# Tufte CSS HTML
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="tufte.css"/>

# NEWPAGE MACRO
#+MACRO: NEWPAGE @@latex: \newpage @@

# FOOTNOTE MACRO
#+MACRO: footnote @@latex: \footnote{$1}@@ @@html: <span class="marginnote">$1</span>@@

# QUOTES
#+MACRO: quote @@html: <blockquote><p>$1</p><footer>$2</footer></blockquote>@@

# FIGURE MACROS
#+MACRO: marginfigure @@latex: \begin{marginfigure} \includegraphics[$1]{$2.pdf}\caption{$3} \end{marginfigure}@@ @@html: <span class="marginnote"><img width="$1" src = "$2.svg" /><br>$3</span>@@
#+MACRO:  figure @@latex: \begin{figure} \includegraphics[$1]{$2.pdf}\caption{$3} \end{figure}@@

# THEOREMS DEFINIIONS ETC
#+MACRO: def @@latex: \begin{def}[$1]\label{$2}$3\end{def}@@


# ALGORITHMS
#+MACRO: algorithm @@html: <img src="$1">@@


# !!!TODO Put Definition, Theorem etc in the thesis-template
# !!!TODO Find out how to export within org file https://emacs.stackexchange.com/questions/31962/how-to-export-latex-snippets-to-html-via-svg-rather-than-png
# !!! TODO on generating tikz inside same document https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-LaTeX.html
# !!! TODO HTML rferences xport  https://emacs.stackexchange.com/questions/62236/org-ref-exporting-org-file-to-html-with-its-style-exactly-same-as-a-specific-sc

# #+BEGIN_EXPORT latex
# \title{Template}
# \newcommand{\subtitle}{KTH Thesis Report}
# \author{<Author Name and Author Name>}
# \setstretch{1.4}

# % The front page of the document
# \pagenumbering{roman}
# \include{setup/title-page}
# \include{sections/0-pre-content}

# \pagenumbering{arabic}
# #+END_EXPORT

# Create org-macros for the following to convert to latex/html when necessary
# \newthough{text}, \footnote{text} (gets converted to sidenote),
# \sidenote[num][offset]{text}, \marginnote{text}, \marginfigure...,
# \margintable..., fullwidth, figure*, figure,
# think of using #+LATEX_HEADER: \input{tufte-book} instead of the init.el file



$\chapter*{Abstract}$
Despite having a philosophical grounding from empiricism that span some centuries, the algorithmization of causal discovery started only a few decades ago. This formalization of studying causal relationships relies on connections between graphs and probability distributions. In this setting, the task of causal discovery is to recover the graph that best describes the causal structure based on the data we receive. A particular class of causal discovery algorithms, called constraint-based methods rely on Directed Acylic Graphs (DAGs) as an encoding of Conditional Independence (CI) relations that carry some level of causal information. However, a CI relation such as $X$ and $Y$ being independent conditioned on $Z$ assumes the independence holds for all possible values $Z$ can take, which can tend to be unrealistic in practice where causal relations are often context-specific.  In this thesis we aim to develop constraint based algorithms to learn causal structure from Context-Specific Independence (CSI) relations within the discrete setting, where the independence relations are of the form X and Y being independent of $Z=a$ for some $a$. This is done by using Context-Specific trees, or CStrees for short, which can encode CSI relations.


{{{NEWPAGE}}}

$\chapter*{Acknowledgements}$
# TODO YOU KNOW ABOVE IS THE UGLIEST FIX YOU DID THIS MONTH

{{{NEWPAGE}}}


#+TOC: headlines:1

{{{NEWPAGE}}}

* Introduction
\label{sec:Intro}
** Motivation
   At the heart of scientific discovery, and thus of modern society, is our ability to gain knowledge about causal relations based on observations and experimentation. Whilst being an active research area for its own sake, causal discovery has many applications to diverse fields ranging from economics  cite:huang-2019-causal-discov and genomics cite:hu-2018-applic-causal to the climate sciences cite:runge-2019-infer-causat. In fact, any field involving any form of measurements/observations which are hypothesized to involve causal interactions can benefit from the tools which this research area has to offer, by allowing the user to get some causal model of the system from which their data is generated. Armed with such a causal model, the user can answer more complex queries than they can using just observed data. Such queries form a hierarchy, referred to as Pearl's Causal Hierarchy, which starts from those related to observations, then interventions, then counterfactuals cite:pearl-2018-book-why. In fact it has been recently shown that their strict distinction holds in the measure theoretic sense cite:elias-2020-pearl-hierar.

** Classical methods
   Historically speaking the task of causal discovery was achieved by using clever experimental design cite:fisher-1935. In particular, Randomized Controlled Trials (RCTs) are often called the gold standard in this regard. The common RCT setting involves allocating subjects in the experiment randomly into 2 groups, where one group called the treatment group receives an intervention and the other group called the control group receives no intervention (or treatment), often in the form of a placebo. One of the main limitations of RCTs is that not every system on which we want to infer causal relationships lends to this setting. For example, it is not a good idea to have a RCT to determine whether smoking causes lung cancer which would involve forcing the subjects to smoke. This creates a need for more sophisticated methods, namely causal models, to learn causal relationships from both observational and interventional data. In fact one can even use such models to explain the success of RCTs in a formal manner.

   
** Context specific causal discovery
   Much of the causal models in wide use today make use of Directed Acyclic Graphs (DAGs), with the semantics that the directed edges represent causal relationships between the variables, which correspond to the nodes in the graph. As we operate under the assumption that any data we receive is generated by some underlying probability distribution, we relate these graphs to them via conditional independence relations which carry elementary causal information about the system of variables. Many algorithms have been proposed to learn such causal models from observed data in recent decades cite:chickering-2002-optimal,spirtes-1991-algor-fast,solus-2021-consis-guaran,tsamardinos-2006-max-min-hill, and are in wide use across various domains. They are, however, restrictive in their modelling capacity, since a conditional independence relation by definition is assumed to hold over all possible outcomes of the conditioning set. We call these outcomes contexts, and it is realistic to assume that causal relations depend on context specific information. A number of approaches exist to model such context specific information cite:collazo-2018-chain,silander-2013,thwaites-2010-causal-analy,goergen-2017-equiv-class, however their representations are not as intuitive as DAGs. Recently, a new representation for context specific causal relations have been proposed, called Context Specific trees (CStrees), which can be seen as a natural generalization of DAGs to the context specific scenario. Namely, CStrees allow for the representation of context specific information as a sequence of DAGs, thus striking a balance between modelling capacity and an intuitive representation. The next natural step is to learn CStrees from data, and this thesis provides the first novel causal discovery algorithms for CStrees to learn from observational data, alongside algorithms to compute the required minimal contexts, which are required for representing CStrees as a sequence of DAGs. This approach is a constraint based approach which generalizes the classic PC algorithm cite:spirtes-1991-algor-fast, alongside applications to both synthetic and real world data.
   

** Relevance to machine learning
   Causal discovery as a subfield of causal modelling contains many ideas which can help in overcoming hard barriers in machine learning. Machine learning can be summarized as the field where practitioners formulate mathematical models of a system of interest, followed by incorporating observed data into this model using various algorithms with the aim of making better predictions about the system. This field has been enjoying significant breakthroughs recently in part due to the availability of a lot of data and faster computers. However, a lot of the work in this field is set in the assumption of independent and identically distributed (i.i.d) data, and ignores information from interventions, domain shifts and temporal structure cite:schoelkopf-2019-causal-machin-learn. As such, there are various problems which still require a causal model, which without it in some cases even give rise to seemingly nuanced paradoxes cite:pearl-2018-book-why, such as Simpsons paradox cite:simpson-1951-inter-inter, where one might for example have a positive correlation between 2 variables over the whole data, but dividing the samples into further groups would result in a negative correlation within each group.  This is not just a theoretical issue, and has been reported in many real life data as well cite:wagner-1982-simps-parad.

    
* Causal Discovery with Directed Acyclic Graphs
** The Causal Discovery problem
We first provide a formalization of the causal discovery problem. Suppose we have a system of $p$ variables $X_1,...,X_p$ which we assume has some underlying probability distribution $\mathbb{P}$, and from which we have $n$ samples $\{x_1^i,...,x_p^i\}_{i=1}^n$. The goal of causal discovery is to recover a structure $\mathbb{G}$ that best represents the causal mechanisms of the system. The structure $\mathbb{G}$ is often a graph with certain properties that enables it to encode information about the system - this means we must make an assumption that such a structure $\mathbb{G}$ exists and it is related to the distribution $\mathbb{P}$. This information about the system is extracted from the samples we have from the distribution $\mathbb{P}$ - this means we have to make further assumptions to relate information we get from samples in $\mathbb{P}$ to our structure $\mathbb{G}$.


The assumptions to be made are an inevitable artefact of the No Free Lunch theorem cite:wolpert-2020-what-no which states that over a uniform distribution over search/learning problems (which includes causal discovery), all algorithms for such problems have equal performance.

# !!! TODO Elaborate further on NFL

There are two common approaches to causal discovery cite:glymour-2019-review-causal. The first is constraint-based methods, which treat the problem of finding the structure as a contraint satisfaction problem. One approach to this is to start from a structure where all variables are causally connected then remove connections based on statistical independence from the observed samples. Second is score based methods, which select a causal representation by assigning a score to all possible models, and then choosing a model that minimizes the score. One approach in this direction is to start from a structure where all variables are not causally connected and then proceed to add connections based on how the observed samples give some score, like the Bayesian Information Criterion (BIC). In this thesis we will mainly be concerned with constraint based methods, particularly in the discrete setting, where we assume the variables in the system can only take discrete values.  



** Direct Acyclic Graphs (DAGs)
We now cover some important definitions and concepts related to Directed Acyclic Graphs (DAGs). They are a convenient and informative graphical means of visualizing the direct cause-effect relationships between variables in a system, and the de-facto choice to model causal structures.
  

#+BEGIN_EXPORT latex

\begin{definition}[DAGs]\label{dagdef}
    A Directed Acyclic Graphic (DAG) is a directed graph $\mathbb{G} = (\mathbb{V},\mathbb{E})$ which has no cycles.
\end{definition}

Let $\mathbb{G} = (\mathbb{V},\mathbb{E})$ be a graph. We then have the following definitions. A node $u \in \matbb{V}$ is a \textbf{parent} of another node $u \in \mathbb{V}$ if $(u,v) \in \mathbb{E}$, in this case we also say $v$ is a \textbf{child} of $u$. If there is an edge $(u,v) \in \mathbb{E}$ or $(v,u)\in \mathbb{E}$ we say the nodes $u$,$v$ are \textbf{adjacent}. A set of nodes $(u_1,...,u_k)$ , $k\geq 2$ such that $(u_i,u_{i+1}) \in \mathbb{E}$ is called a path between $u_1$ and $u_k$. This in case, we say $u_1$ is an \textbf{ancestor} of $u_k$ and $u_k$ is a \textbf{descendant} of $u_1$. If we have a node $v$ that is not a descendant of a node $u$ we say $v$ is a \textbf{non-descendant} of $u$.


For any node $u \in \mathbb{V}$ we denote $PA_{\mathbb{G}}(u)$, $CH_{\mathbb{G}}(u)$, $DS_{\mathbb{G}}(u)$, $ND_{\mathbb{G}}(u)$ to be set of parents, children, descendants and non-descendants of $u$ respectively.

\end{definition}



#+END_EXPORT

{{{marginfigure( width=\linewidth, ./figures/dageg, Example of a DAG $\mathbb{G}=(\mathbb{V}\,\mathbb{E})$ with $\mathbb{V} = \{1\,2\,3\,4\,5 \}$ and $\mathbb{E} = \{(1\,2)\,(1\,3)\,(1\,5)\,(2\,4)  \}$. Here $PA_{\mathbb{G}}(2)=\{1\}$\, $DS_{\mathbb{G}}(2)=CH_{\mathbb{G}}(2)=\{1\}$\, $ND_{\mathbb{G}}(2)=\{3\,5\}$ )}}}
# TODO !!! Throughout this paper we will use $\mathbb{V} = \{1,...,p\} =[p]$ where $p$ is the number of variables in the system of interest, thereby mapping the possibly qualitative variables of the system into numbers for notational ease.
{{{marginfigure( width=\linewidth, ./figures/dagneg, This graph is not a DAG since there is a cycle)}}}


Since we a working with discrete probability distributions, we introduce the (open) probability simplex as the space of all possible probability distributions over a set of discrete variables $X_1,...,X_p$ whose outcomes are elements of $\mathcal{X}=\prod_{i=1}^p \mathcal{X}_i$.

#+begin_export latex
\begin{definition}[Probability simplex]\label{probsimplex}
Given a finite set $\mathcal{X}$, The probability simplex on this set is \\ $\Delta_{|\mathcal{X}|-1} = \{ (f_x \, : x \in \mathcal{X}) \in \mathbb{R}^{|\mathcal{X}|} \, : \, \forall x \in \mathcal{X} \; f_x > 0, \, \sum_{x\in \mathcal{X}}f_x =1\}.$
\end{definition}
#+end_export

Each point in the probability simplex corresponds to a joint distribution over $(X_1,...,X_p)$, and our interest mainly lies to the subset of of this space which are connected to structures we can use to model causal relations.


An important concept when relating DAGs to distributions is that of conditional independence, which we define below.
#+begin_export latex
\begin{definition}[Conditional Independence]\label{def:cirel}
Let  $\mathbb{P}$ be a distribution with variables $X_1,...,X_p$. Given non-empty subsets $A,B \subset [p]$ and a (possibly empty) subset $S \subset [p]$ such that $\mathbb{P}(X_B, X_S)>0$ and $A \cap B \cap S = \{\}$, we say the variables $X_A$ and $X_B$ are conditionally independent given $S$, (denoted $(X_A\indep_{\mathbb{P}} X_B \,|\, X_S)$) if $\mathbb{P}(X_A, \,|\,X_B, X_S) = \mathbb{P}(X_A \, |\, X_S)$ holds for all possible outcomes of $X_A,X_B,X_S$.
\end{definition}
#+end_export

The conditional independence statement $(X_A \indep_{\mathbb{P}} X_B \,|\,X_S)$ can be viewed as a ternary relation on $X_A,X_B,X_S$, and is called a Conditional Independence (CI) relation. This relation formalizes the concept of $X_B$ and $X_A$ not providing any information when we have observed $X_S$, which is to say, if we already know $X_S$, knowing $X_B$ does not change the probabilities for $X_A$, and vice versa.

# !!! TODO Example of CI relationmodel, the graphoid maybe


Using this we can now define the local Markov property which relates distributions to DAGs based on the CI relations encoded by them. As the CI relations have a natural causal interpretation, the local Markov property provides a foundation to relate data generating distributions to DAG representations of a causal system.


#+begin_export latex
\begin{Definition}[Local Markov property]\label{thm:localmarkovdag}
Let $\mathcal{G}$ be a DAG with nodes $[p]$. A probability distribution $\mathbb{P}$ satisfies the local Markov property with respect to $\mathbb{G}$ if for each node $i \in [p]$, the variable representing that node, $X_i$ is independent of its non-descendants when conditioned on its parents, formally, $(X_i \indep X_{ND_{\mathbb{G}}(i)}\,|\,X_{PA_{\mathbb{G}}(i)})$
\end{definition}
#+end_export

This formalizes the fact that in order to computationally generate data from a DAG $\mathbb{G}$, the value of each variable $X_i \in \mathcal{X}_i$ depends only on the values of the outcomes of its parents in $\mathbb{G}$. This means that for a (discrete) distribution $\mathbb{P}$ with $p$ variables satisfying the Local Markov property, the distribution can be encoded with $p$ probability tables which give the probabilities for each $X_i$ taking a value when conditioned on all possible outcomes of its parents. From a storage perspective, this means we have to store $\sum_{i=1}^p |\mathcal{X}_i| |\prod_{j \in PA_{\mathbb{G}}(i)}\mathcal{X}_j |$ which is significantly smaller than having to store all possible probability values which would require one table with $|\prod_{i=1}^p |\mathcal{X}_i|$ values. For binary variables assuming $d$ parents for each variable, this is the difference between $p2^{d+1}$ and $p2^p$.


For the purposes of this thesis, it is worth introducing the Ordered Markov property which uses the concept of a linear ordering. {{{footnote(For a DAG \mathbb{G} with $p$ nodes a linear ordering is an ordering of the nodes that respects the directions in $\mathbb{G}$ that is each node $i$ always comes after each $j \in PA_{\mathbb{G}}(i)$. It is a  also called a topological ordering, and later on we will use this ordering as a causal ordering for events.)}}}

#+begin_export latex
\begin{definition}[Ordered Markov Property]\label{orderedmarkov}
Let $\mathbb{G}$ be a DAG and $\pi = \pi_1 \cdots \pi_p$ a causal ordering of $\mmathbb{G}$. A probability distribution $\mathbb{P}$ satisfies the Ordered Markov property with respect to $\mathbb{G}$ if we have $(X_i \indep X_{\{1,...,i-1 \} \textbackslash PA_{\mathbb{G}}(i)}\,|\, X_{PA_{\mathbb{G}}(i)})$ 
\end{definition}
#+end_export

A distribution $\mathbb{P}$ satisfying the local Markov property with respect to a DAG $\mathbb{G}$ is equivalent to that distribution also satisfying the ordered Markov property with respect to $\mathbb{G}$ and a linear ordering of $\mathbb{G}$.

# TODO name everything linear ordering

    # TODO Think about this The Ordered Markov property is a way to restate the local Markov property under the assumption that we know a causal ordering of the variables.


An important notion in DAGs is that of d-separation and blocked paths.
 {{{footnote( \baselineskip \baselineskip A path between 2 nodes is any set of edges connecting them irrespective of the direction.)}}}.


# !!! TODO Add path and d-sep example from above DAG after changing the labels
# !!! Analyse triples or consecutive triples?
#+begin_export latex
\begin{definition}[Blocked path]\label{bpath}

Given a DAG $\mathbb{G}$, and a path between nodes $i,j \in \mathbb{V}$, we say the \textbf{path is blocked} by a (potentially empty) set of nodes $S$ if either of the following hold:
\begin{itemize}
\item Along the path there is a triple of nodes $(x,s,y)$ such that $x \rightarrow s \rightarrow y$, $x \leftarrow s \leftarrow y$, or $x \leftarrow s \rightarrow y$ with $s \in S$
\item Along the path there is a triple of nodes $(x,s,y)$ such that $x \rightarrow s \leftarrow y$ such that $s \notin S$ and no descendants of $s$ are in $S$.
\end{itemize}

\end{definition}


\begin{definition}[d-separation]\label{def:dsep}

Given a DAG $\mathbb{G}$,  two (non-empty) sets of nodes $X,Y$ are \textbf{d-separated} by a (potentially empty) set of nodes $S$ in $\mathbb{G}$, denoted $(X\indep_{\mathbb{G}}Y\,|\,S)$ if all paths between every node in $X$ and every node in $Y$ are blocked by $S$. 

\end{definition}
#+end_export

# !!! TODO Describe the 3 building blocks

{{{marginfigure(width=\linewidth, ./figures/chainl, Chain )}}}
# {{{marginfigure(width=\linewidth, ./figures/chainr, )}}}
{{{marginfigure(width=\linewidth, ./figures/fork, Fork/Common cause)}}}
{{{marginfigure(width=\linewidth, ./figures/collider, V-structure/ Collider/Immorality)}}}


# terminology, forks, chains, immoralities

# The moment a path is inactive/blocked triple, the entire path is inactive/blocked
# All paths must be blocked to guarantee independence
# Active paths correspond to a path where information can flow, thus no guarantee of independence, the moment we see an active path we cannot guarantee this indepdence


The notion of d-separation relates DAGs to probability distributions from the following theorem.
#+begin_export latex
\begin{definition}[Global Markov property]\label{thm:dagci}

Given a distribution $\mathbb{P}$ that satisfies the local Markov property with a DAG $\mathbb{G}$, we have that for any (non-empty) sets $A,B$ and (possibly empty) set $S$, $(X_A \indep_{\mathbb{G}} X_B \,|\,X_S) \implies (X_A \indep_{\mathbb{P}} X_B \,|\, X_S)$


\end{definition}
#+end_export

An important result is the following that the above notions are indeed equivalent cite:duarte-2020-algeb.

#+begin_export latex
\begin{theorem}[Markov theorems for DAGs]\label{thm:markovdag}
Given a distribution $\mathbb{P}$ over $X_1,...,X_p$ and a DAG $\mathbb{G}$ over $p$ nodes, the following are equivalent

\begin{itemize}
\item $\mathbb{P}$ is Markov to $\mathbb{G}$ i.e. $\mathbb{P}(X_1,...,X_p) = \prod_{i=1}^p \mathbb{P}(X_i \, |\, X_{PA_{\mathbb{G}}(i)})$
\item $\mathbb{P}, \mathbb{G}$ satisfy the local Markov property
\item $\mathbb{P}, \mathbb{G}$ satisfy the ordered Markov property
\item $\mathbb{P}, \mathbb{G}$ satisfy the global Markov property
\end{itemize}

\end{theorem}
#+end_export


If $\mathbb{P}$ satisfies the local Markov property with respect to $\mathbb{G}$ and has a probability density with respect to a product measure, we say $\mathbb{P}$ is Markov with respect to $\mathbb{G}$, or equivalently, $\mathbb{G}$ is an Independence map (I-MAP) of $\mathbb{P}$ cite:lauritzen-1996-graph.

Thus DAGs can be used to store Conditional Independence (CI) relations. More importantly, d-separation encodes the complete set of CI relations satisfied by all distributions Markov to a DAG, i.e. distributions that are Markov to a DAG $\mathbb{G}$ *and* satisfy *exactly* the CI relations encoded by d-separation exist cite:meek-2013-stron-compl,geiger1990identifying.

# !!! TODO Introduce CI Models as a subset of the simplex, exmaple of how difference A_|_B and A_|_B|C are

It is also possible to have 2 DAGs that encode the same CI relations, in which case we say that they are both in the same Markov Equivalence Class (MEC), and we say they are Markov Equivalent. MECs can be characterized by the following theorem cite:verma-2013-equiv-causal-model.

#+begin_export latex
\begin{theorem}[Characterization of MECs]\label{thm:vermapearl}
Two DAGs $\mathbb{G}_1$ and $\mathbb{G}_2$ are Markov Equivalent if and only if they have the same skeleton (underlying undirected edges) and v-structures, where a v-structure is a triple of nodes $(i,j,k)$ with edges $i \rightarrow j \leftarrow k$ and $i,k$ do not share an edge.
\end{theorem}
#+end_export

For example, the Chain and Fork graphs from the previous page belong to the same Markov Equivalence class.

** Causal Discovery Algorithms for DAGs

   Theorem \ref{thm:markovdag} suggests that we can make use of CI testing on a distribution $\mathbb{P}$ to learn a DAG \mathbb{G}. However, the distribution $\mathbb{P}$ may contain CI relations not encoded in the DAG, thus we make the following assumption.

#+begin_export latex
\begin{definition}[Faithfulness]\label{def:faithfulness}

A probability distribution $\mathbb{P}$ is faithful to a DAG $\mathbb{G}$ if it entails only the CI relations encoded by the d-separations in the DAG.

\end{definition}
#+end_export

Under the faithfulness assumption, the global Markov property holds both ways. It should be noted that faithful distributions exist cite:meek-2013-stron-compl, and the set of distributions that are not faithful to a dag $\mathbb{G}$ have measure $0$ cite:uhler-2013-geomet-faith, which suggets that in theory this is not a very restrictive assumption.



One of the first practical algorithms which make use of the theory above is the PC algorithm, cite:spirtes-2000-causation-prediction-search,kalisch-2007-estim-high which is a constraint based causal discovery algorithm that relies of the characterization of DAGs in Theorem \ref{thm:vermapearl} and the faithfulness assumption to find a DAG in the MEC of the true causal DAG. The algorithm starts from a complete graph and runs conditional independence tests to first find the DAG skeleton and then proceeds to direct the edges whenever possible. The output of the PC algorithm is a Completed Partially Directed Acyclic Graph (CPDAG) cite:meek-2013-causal-infer, which acts as a representation for the Markov Equivalence class. A Partially Directed Acylic Graph (PDAG) is a graph where some edges are directed and some are undirected and there is no cycle in the direction of the directed edges and any direction of the undirected edges. A PDAG a is Complete PDAG (CPDAG) if every directed edge exists also in every DAG in the Markov Equivalence class of the DAG and for every undirected edge between nodes $i,j$ there exists a DAG with the edge $i \rightarrow j$ and a DAG with $j \rightarrow i$ in the equivalence class. CPDAGs are also sometimes called essential graphs cite:andersson-1997-charac-markov.



# !!! With the assumption that existence of edges means causal relation, blabla, we can get skeleton!!! To direct edges we can make use of a characterization of DAGs




   # !!! TODO Assumptions in PC Algorithm (causal sufficiency, faithfulness, causal Markov assumption)



   


** Limitations of using DAGs
   DAGs are a simple and informative structure for causal discovery, however their ability to only encode CI relations is a limitation. This is because the CI relation  $(X_A \indep_{\mathbb{P}} X_B \,|\, X_S)$ implies that $X_A$ and $X_B$ are independent for all possible outcomes of $X_S$, which in some cases might be too strong of an assumption. A generalization of such relations is Context Specific Independence (CSI) relations, defined below.
   #+begin_export latex
\begin{definition}[Context Specific Independence]\label{def:csirel}
Let  $\mathbb{P}$ be a distribution with variables $X_1,...,X_p$ with a state space $\mathcal{X} = \prod_{i=1}^p \mathcal{X}_i$. Given non-empty subsets $A,B \subset [p]$ and (possibly empty) subsets $S,C \subset [p]$ and $x_C \in \prod_{i \in C}\mathcal{X}_i $ such that $\mathbb{P}(X_B, X_S, X_C = x_C)>0$ and $A \cap B \cap S \cap C = \{\}$, we say the variables $X_A$ and $X_B$ are conditional independent given $S$, in the context $X_C=x_C$ (denoted $(X_A\indep_{\mathbb{P}} X_B \,|\, X_S)$) if $\mathbb{P}(X_A \,|\,X_B, X_S,X_C=x_C) = \mathbb{P}(X_A \, |\, X_S,X_C=x_C)$ holds for all possible outcomes of $X_A,X_B,X_S$.
\end{definition}
   #+end_export


   In the next chapter we introduce Context Specific Trees (CStrees) which can encode such relations, and thus provide a structure that can capture the context specific information glossed over in DAGs.
   

{{{NEWPAGE}}}

* Causal Discovery with Context Specific Trees
One intuition is that to capture context specific relations one needs to make use of a structure that explicitly represents separate outcomes of a distribution. Typically in high school some might have encountered the use of trees to model small probabilistic systems, and they fully include all possible outcomes involved, and serve as an important tool to compute probabilities for relevant events. As we will see in this chapter, this is a good way to approach the problem of encoding context information as well.

** Context Specific Trees (CStrees)
   Before defining CStrees we start by defining staged trees, which contain CStrees as a subset. Both of these are rooted trees. {{{footnote(A rooted tree $\mathbb{T} = (\mathbb{V}\,\mathbb{E})$ is a directed graph whose skeleton is a tree and there exists a unique node $r$ such that $PA_{\mathbb{T}}(r) = \{\}$ which is called the root.)}}}
   #+begin_export latex
   \begin{definition}[Staged trees]
   Let $\mathbb{T} = (\mathbb{V},\mathbb{E})$ be a rooted tree, $\mathbb{L}$ a finite set of labels for the edges, and $\theta : \mathbb{E} \rightarrow \mathbb{L}$ a labelling of the edges. Let $E_{\mathbb{T}}(v) = \{v \rightarrow w \in \mathbb{E} \,:\, w \in CH_{\mathbb{T}}(v) \}$,   i.e. the set of edges coming out of $v$ in $\mathbb{T}$. The pair $(\mathbb{T}, \theta)$ is a staged tree if 
\begin{itemize}
\item  $\forall v \in \mathbb{V}$ we have |$\theta(E_{\mathbb{T}}(v))$| = |$E_{\mathbb{T}}(v)$|
\item $\forall v,w \in \mathbb{V}$ we have that both $\theta(E_\mathbb{T}(v))$ and $\theta(E_\mathbb{T}(w))$ are either equal or disjoint
\end{itemize}
\end{definition}
#+end_export

This can be thought of as a probability tree where each edge represents a probability value, and the probabilities coming out of all edges from any given node sum to 1. More formally, first define the space of canonical parameters of the staged tree $(\mathbb{T},\theta)$ as

#+begin_export latex
$\Theta_{\mathbb{T}} = \{  x\in \mathbb{R}^{|\mathbb{L}|} \, : \, \forall e \in \mathbb{E}, x_{\theta(e)}\in (0,1), \forall v \in \mathbb{V}, \, \sum_{e \in E_{\mathbb{T}}(v)} x_{\theta(e)}=1 \}$.
#+end_export

Given the probability simplex $\Delta_{|\mathcal{X}|-1}$ and letting $\mathbf{i}_{\mathbb{T}}$ be the set of all leaves of the staged tree $\mathbb{T}$  the staged tree model is defined as below.


#+begin_export latex
\begin{definition}[Staged tree models]\label{def:stagedtreemodel}
The staged tree model $\mathbb{M}_{(\mathbb{T},\theta)}$ is the image of the map $\varphi_\mathbb{T} \, : \, x \rightarrow f_v := $ $\Big($ $\prod_{e \in E_{\mathbb{T}(\lambda(v))} x_{\theta(e)}$ $\Big)_{v \in \mathbf{i}_{\mathbb{T}}}$
\end{definition}
#+end_export

Thus given variables $X_1,...,X_p$, a causal ordering $\pi$, the staged tree for this with levels {{{footnote(The $k^{th}$ level of a rooted tree\, $L_k$\, is the set of nodes such that the unique path from each node in $L_k$ to the root consists of $k$ edges.)}}} $L_1,...,L_p \sim X_{\pi_1},...,X_{\pi_p}$, each path from the root to the leaf defines a sequence of events $x_1, x_1x_2, ...,x_1\cdots x_p$ where $x_i \in \mathcal{X}_{\pi_i}$. Since for the edge  $e = ((x_1\cdots x_k), (x_1\cdots x_kx_{k+1}))$ we have $x_{\theta(e)} = \mathbb{P}(x_{k+1}\,|\, x_1\cdots x_k)$, the product in Definition \ref{def:stagedtreemodel} does indeed result in $\mathbb{P}(v_1,...,v_p)$ for each $v \in \textbf{i}_{\mathbb{T}}$ by the chain rule in probability. {{{footnote(The chain rule in probability states $\mathbb{P}(X_1 \,...\,X_p) = \mathbb{P}(X_p \; |\; X_{p-1}\,...\,X_1)\mathbb{P}(X_{p-1} \; | \; X_{p-2}\,...\,X_1 )\\ \cdots \mathbb{P}(X_2|X_1)\mathbb{P}(X_1)   )}}}

# !!! TODO Dont fully understand the above x_theta thing

The important characteristic of staged trees are the stages. 

#+begin_export latex
\begin{definition}[Stages]

Given a staged tree $(\mathbb{T},\theta)$, we say two nodes $v,w$ are in the same stage if and only if  $\theta(E_\mathbb{T}(v)) = \theta(E_\mathbb{T}(w))$

\end{definition}
#+end_export


Stages are represented by colours, and when a stage contains a single node, it is coloured white. Staged tree models generalize DAG models, i.e. distributions represented by DAGs, however they are perhaps too general, in the sense that despite allowing for the representation of context specific information, they do not admit a intuitive representation of the causal structure. This creates the need for a structure that generalizes DAG models *and* admits an intuitive representation. The recently proposed subclass of staged trees, known as CStrees allow for this.

#+begin_export latex
\begin{definition}[CStrees]\label{def:cstree}
Let $\mathcal{X}_i$ denote the state space of some variable $X_i$ with $\mathcal{X} = \Pi_{i=1}^p \mathcal{X}_i$, and $(\mathbb{T},\theta)$ be a staged tree with levels $L_1,...,L_p$ corresponding to variables $X_{\pi_1},...,X_{\pi_p}$ where $\pi = \pi_1...\pi_p$ is the causal ordering of the variables.  
A CStree is a staged tree $(\mathbb{T}, \theta)$ where each level of the tree corresponds to some variable and  such that 
\begin{itemize}
\item It is compatibly labelled, i.e. $\forall x_{\pi_k} \in \mathcal{X}_{\pi_k}$ we have $\theta(x_{\pi_1}...x_{\pi_{k-1}}\rightarrow x_{\pi_{k-1}}x_{\pi_k}) = \theta(y_{\pi_1}...y_{\pi_{k-1}}\rightarrow y_{\pi_{k-1}}x_{\pi_k})$ whenever $x_{\pi_1}...x_{\pi_{k-1}}$ and $y_{\pi_1}...y_{\pi_{k-1}}$ are in the same stage
\item (\textbf{CStree property}) Each stage $S_i \subset L_k$ of the tree has a fixed context, i.e. $\exists C_i \subset [k]$ and the fixed outcome $x_{C_i} \in \mathcal{X}_{C_i}$, where the stages are the union over the variables beside those in $C_i$, i.e. if $Y_i = [k] \textbackslash C_i$ then $S_i = \bigcup_{x_{Y_i} \in \mathcal{X}_{Y_i}} \{x_{C_i}x_{Y_i} \}$  
\end{itemize}
\end{definition}
   #+end_export 

   # !!! TODO read discussion after def 3.1 in liams paper and talk more about cstrees here



Given a CStree $\mathbb{T}$ and a causal ordering $\pi$, each node in level $L_k$ corresponds to an outcome of the sequence of variables $X_{\pi_1},...,X_{\pi_k}$. Each edge coming into each node in $L_k$ is of the form $(x_1\cdots x_{k-1},x_1\cdots x_k)$ represents $P(x_{k}|x_1 \cdots x_{k-1})$, which is also the value of the parameter associated to this edge. Suppose we fix a node $n = a_1\cdots a_k \in L_k$. Each edge coming out of $n$ gives the probabilities for the variable in the next level $L_{k+1}$, conditioned on the context $(X_{\pi_1}=a_1,...,X_{\pi_k}=a_k)$. Thus, we can view this node $n$ as containing the distribution $\mathbb{P}(X_{\pi_{k+1}}\,|\, X_{\pi_1}=a_1,...,X_{\pi_k}=a_k)$ This is an important view which we will make use of when testing for context specific independence in the algorithms throughout this paper. We show an example of a CStree and a staged tree that is not a CStree below.



      #+begin_export latex
\begin{figure}[!h]\label{fig:cstreestagedtree}
   \begin{floatrow}
\ffigbox{\includegraphics[width=0.95\linewidth]{figures/cstreestagedtree.pdf}}%
\caption{Example of a staged tree model that is not a CStree (Left) and a CStree (right) for binary variables $X_1,X_2,X_3,X_4$ in that causal ordering.}
        
   \end{floatrow}
\end{figure}
   #+end_export

Both staged trees in Figure \ref{fig:cstreestagedtree} represent 4 binary variables $X_1,X_2,X_3,X_4$ taking values in $\{0,1\}$ in that causal order. Suppose each edge to the left corresponds to the outcome $0$ and the other corresponds to one. In this case, the left edge coming out of the root represents $\mathbb{P}(X_1 = 0)$ and the right edge coming out the root represents $\mathbb{P}(X_1 = 1)$. The nodes represent distributions conditioned on the context unique to them. For example, the left most red node in both trees represent $\mathbb{P}(X_3 \,|\, X_2=0, X_1=0)$. The tree on the right is a CStree because each of the nodes in the non-singleton stages, which are represented by a non-white colour, share exactly one fixed context. For example, the stage corresponding to the blue nodes in the tree on the right (the CStree) corresponds to the contexts $(X_1=1, X_2=0, X_3=0), (X_1=1, X_2=1, X_3=1), (X_1=1, X_2=1, X_3=0), (X_1=1, X_2=1, X_3=1)$. The common context for this stage is  $(X_1=1)$. Meanwhile, for the tree on the left, the stage corresponding to the blue nodes only share the empty context, meaning all nodes in level 3 must correspond to the stage with the empty context for it to be a CStree - this is however not the case since there are nodes in level 3 which correspond to the yellow and green stages, thus not part of the blue stage. 


A CStree encodes Context Specific Independence (CSI) relations according to the following lemma cite:duarte-2021-repres-learn


#+begin_export latex
\begin{lemma}[CStrees and Context Specific Independence relations]\label{lem:cstreecsi}

Let $\mathbb{T} = (\mathbb{V},\mathbb{E})$ be a CStree with levels $X_1,...,X_p \sim L_1,...,L_p$ and stages $S_1,...,S_m$. Then for any $\mathbb{P} \in \mathbb{M}_{(\mathbb{T},\theta)}$ and $S_i \subset L_{k-1}$, $\mathbb{P}$ entails the CSI relation $(X_k \indep_{\mathbb{P}} X_{[k-1] \textbackslash C_i} \, | \, X_{C_i} = x_{C_i})$ where $X_{C_i}=x_{C_i}$ is the context fixed by the stage $S_i$.

\end{lemma}
#+end_export

The CSI relations from the CStrees look similar to the CI relations from the ordered Markov property. The difference is that the CStree encodes independence of $X_k$ with all the variables preceding it in the causal ordering when conditioned on a context, compared to the ordered Markov property which contains the variables which represent the parents of $k$ in the conditioning set. Thus, CStrees can be thought of as a relaxation of DAG models via a relaxation of the ordered Markov property, where we condition on the more general scenario of contexts, rather than variables. 



** Learning CStrees from observed data
   Given a system of variables $X_1,...,X_p$, we would first need a causal ordering $\pi_1 \cdots \pi_p$ in order to construct a CStree for these variables. Since CStrees encode CSI relations, they can also encode CI relations, which means we can generate a CStree from a DAG. The following proposition formalizes this notion cite:duarte-2020-algeb.

   #+begin_export latex
\begin{proposition}[CStrees corresponding to DAGs]\label{prop:dagandcstree}
A compatibly labelled staged tree $\mathbb{T}$ with causal ordering $\pi_1 \cdots\pi_p$, levels $L_1,...,L_p$ corresponding to variables $X_{\pi_1},...,X_{\pi_p}$ encodes the same CI relations as some DAG $\mathbb{G}$ if and only if for any topological ordering of $\mathbb{G}$, $\forall k \in [p-1]$, the level $L_k$ has its nodes partitioned into stages where the context for each stage is an element of the Cartesian product of the parents of $X_{\pi_{k+1}}$ in $\mathbb{G}$.
\end{proposition}
   #+end_export

   We describe the computational procedure to generate a CStree $\mathbb{T}$ from a DAG $\mathbb{G}$ below, assuming that we are given a causal ordering of $\mathbb{G}$. {{{footnote(\textsc{Parents} is a function that takes a graph and a node and returns the parents of that node in the graph; \textsc{CartesianProduct} takes a set of variables and returns the cartesian product of these variables i.e. all possible values they can take)}}}.


   
#+NAME: alg:1
#+BEGIN_EXPORT latex
\begin{algorithm}[H]
\label{alg:dagtocstree}
      \SetAlgoLined
      \KwIn{A DAG $G$, causal ordering $O$}
      \KwOut{CStree $T$ with ordering $O$ and stages $S$ defined by $G$}
      $T \leftarrow$ Empty staged tree with ordering $O$\;
      $S \leftarrow$ Empty dictionary\;
       \For{$l$ in $|O|-1$}{
        $v \leftarrow O[l+1]$ \;
	$T.add\_level(v)$\;
	$a \leftarrow \;  \textsc{Parents}(G, v)$\;
	$b \leftarrow$ \textsc{CartesianProduct}($a$)\;
	\tcp{Each element of $b$ is a context which fixes a stage in level $l$}
	\For{$c$ in $b$}{
	$S[c] \leftarrow$ [nodes in level $l$ such that $c$ is a subcontext]\;
	}
       }
       \caption{\textsc{DagToCStree}\\Constructing a CStree from a DAG}
       \KwRet{$T, S$}
      \end{algorithm}
#+END_EXPORT

Algorithm \ref{alg:dagtocstree} above does not necessarily need a causal ordering. This is because given a DAG we can perform a topological sort on it to get one, for which efficient algorithms exist cite:tarjan-1976-edge-disjoin.

#+begin_export latex
\begin{theorem}\label{thm:dagtocstreecorrectness}
Given variables $X_1,...,X_p$ taking values in $\mathcal{X}=\prod_{i=1}^p \mathcal{X}_i$ , Algorithm \ref{alg:dagtocstree} is correct and runs in $\mathcal{O}(d^{2p})$ time and $\mathcal{O}(d^p)$ space where $d = \max_{i \in [p]} |\mathcal{X}_i|$.
\end{theorem}

\textit{Proof:
	For correctness, at each level $L_k$, the non-singleton stages are created for the contexts fixed by the outcomes of the parents of $X_{\pi_{k+1}}$ thus by Proposition \ref{prop:dagandcstree} the tree is still a CStree. Since the staging process at each level only creates non-singleton stages of nodes within that level, and we go over each level except the last level which always contain singleton stages (one for each outcome of $\mathcal{X}$), the stages $S$ lead to $T$ being a CStree. For time complexity, the worst case scenario is for the fully connected DAG, assuming the ordering $12\cdots p$, node $i$ has $i-1$ parents. This however results in a CStree with no non-singleton stages. Thus we look at the scenario where node $i$ has $i-2$ parents. At the level for the variable representing node $i$, the variable b in Algorithm \ref{alg:dagtocstree} which is all the elements of the the Cartesian product of values the parents take, has $|\prod_{j=1}^{i-2} \mathcal{X}_j|$ elements. For each element in this Cartesian product which fixes the context for the stage, we have to loop over all nodes in level $i$ and to store the nodes for that stage, and level $i$ has $|\prod_{j=1}^i \mathcal{X}_j|$ nodes. Thus the loop for level $i$ takes $|\prod_{j=3}^{i-2} \mathcal{X}_j ||\prod_{j=1}^i \mathcal{X}_j| $ where the indexing starts at 3 for the first term since the parent sets are non-empty starting from node 3. Since we have $p$ levels, ignoring the first 2 since their variables have no parents, we have}

	\begin{align*}\sum_{i=3}^p |\prod_{j=3}^{i-2} \mathcal{X}_j ||\prod_{k=1}^i \mathcal{X}_k| < \sum_{i=1}^p |\prod_{j=1}^i |\mathcal{X}_j||\prod_{k=1}^i |\mathcal{X}_k|< \sum_{i=1}^p \prod_{j=1}^i d \prod_{k=1}^i d 
	\end{align*}

\textit{where $d = \max_{i \in [p]} |\mathcal{X}_i|$. This sum then becomes }
\begin{align*}
\sum_{i=1}^p d^{2i}  = \frac{d^2 (d^{2p}-1)}{d^2-1} = \mathcal{O}(d^{2p})
\end{align*}

\textit{For space complexity, in the worst case DAG mentioned, level $i$ which has $\prod_{j=1}^i |\mathcal{X}_j| < d^i$ nodes and the same amount of edges coming in. For storing the stages, the extra information we need to store is the fixed contexts for each stage, and  there are $\prod_{j=3}^i |\mathcal{X}_j| < d^i$ stages in level $i$. Thus the nodes, edges and stages for level $i$ are at most $3d^i$, summing for each level gives}
\begin{align*}\sum_{i=1}^p 3d^i= \frac{3d (3d^{p}-1)}{3d-1} = \mathcal{O}(d^{p})\end{align*}
}
#+end_export

We mention the space complexity here to emphasize that it grows exponentially, which is one limitation of this approach. For $p$ binary variables this means a CStree takes $\mathcal{O}(2^p)$ space. This is in comparison to DAGs which in the worst case assuming full connectivity require $\mathcal{O}(p^2)$ space, independent of the state space of the variables. 


In order to learn CSI relations, one can now take a CStree from a DAG and perform a statistical test to determine context specific independence relations. Recall that each node in level $k$ represents a probability density of the variable in level $k+1$ under the context fixed by that node. Thus for each level, we can compare all possible pairs of nodes by taking the samples fixed by the contexts of the pair, and testing whether they are from the same distribution. If so, we assign the same colour to both of them. Then by the CStree property from Definition \ref{def:cstree} we must have that all nodes in level $k$ which share the same context as that of these 2 nodes must also have the same colour. For example with binary variables if we have 2 nodes representing the outcomes $X_{\{1,2,3,4\}}=0110, X_{\{1,2,3,4\}}=0011$ {{{footnote($X_{\{1\,2\,3\,4\}}=0110$ is shorthand for $(X_1=0\, X_2=1\, X_3=1\,X_4=0)$)}}}  and we know they are in the same stage $S_i$, then the common context for that stage is $X_{\{1,4\}}=01$, and by the CStree property all nodes in that level with this subcontext belong to the same stage. 


# !!! TODO More on the Cstree property as an assumption

{{{NEWPAGE}}}
We now describe the algorithm for learning a CStree. {{{footnote(\textsc{Colour} is a function that takes a node and returns the colour of it if it belongs to a non-singleton stage - note here we represent the stage using a colour; \textsc{CommonContext} is a function that takes 2 nodes and returns their common context - if one or both of them already belong to a stage\, we take this to be the common context between these contexts; \textsc{Test} is a function that determines whether the distributions corresponding to both of the nodes belong to the same stage or not - this typically involves a statistical test;  \textsc{NodesWithContext} takes a set of nodes and a context $c$ and returns the nodes which have the $c$ as a subcontext; \textsc{UpdateStages} is a function that updates the stages of the tree with the new nodes.)}}}

#+BEGIN_EXPORT latex
\begin{algorithm}[H]\label{alg:learncstree}
\SetAlgoLined
\KwIn{CStree $T$, (possibly empty) stages $S$, causal ordering $O$, Data matrix $D$}
\KwOut{The CStree $T$ with ordering $O$ and stages $S$}
$l=1$\;
$p=|O|$\;
\While{l < $p$}{
    $ns \gets$ [nodes in level $l$ of $T$]\;
    $ps \gets$ [all pairs of nodes in level $l$]\;
    \For{ $(n_1,n_2)$ in $ps$}{
    \eIf{\textsc{Colour}($n_1$)=\textsc{Colour}($n_2$)}
        {skip}
	{
    $c \gets$ \textsc{CommonContext}($n_1,n_2$)\;
    $same\_distr =$ \textsc{Test}($c, n_1,n_2, \: D, \: O[l+1]$)\;
    \If{same\_distr}
    {
        $new\_nodes \gets$ \textsc{NodesWithContext}($ns,c$)\;
	$S \gets$ \textsc{UpdateStages}($S$, $c$, $new\_nodes$)\;
    }
    }
}
$l=l+1$\;
}
\caption{\textsc{LearnCStree} \\ Learning a CStree with knowledge of causal ordering}
\KwRet{$T,S$}
\end{algorithm}
#+END_EXPORT

# !!! On choosing pair of nodes, reseviour sampling, node size increasing etc

Algorithm \ref{alg:learncstree} can be sped up by already providing a non-empty CStree containing stages which we may have inferred from a DAG. If one knows the DAG and the true causal ordering of the system they can learn a CStree by using Algorithm \ref{alg:dagtocstree} followed by Algorithm \ref{alg:learncstree}.


#+begin_export latex
\begin{theorem}\label{thm:learncstreecorrectness}
Given variables $X_1,...,X_p$ taking values in $\mathcal{X}=\prod_{i=1}^p \mathcal{X}_i$ , Algorithm \ref{alg:learncstree} is correct and runs in $\mathcal{O}(d^{2p})$ time assuming constant time for statistical independence testing, where $d=\max_{i \in [p]}\mathcal{X}_i$.

\end{theorem}

\textit{Proof:
For correctness, at level loop iteration we compare all pairs of nodes and only update the stages if they do not belong to the same non-singleton stage. In this case if they do belong to the same stage according to the statistical testing, we add exactly the nodes that belong to the stage according to the CStree property. Thus the CStree property is intact throughout the algorithm. For time complexity, using notation from Theorem \ref{thm:dagtocstreecorrectness}, level $i$ has $d^i$ nodes and in the worst case we run statistical testing on all pairs of nodes, of which there are ${d^i \choose 2} = \frac{d^i !}{2! (d^i - 2)!} < \frac{d^{2i}}{2}$, summing for each level gives $\sum_{i=1}^p \frac{d^{2i}}{2}  = \mathcal{O}(d^{2p})$.
}
#+end_export

In the general case it is possible that the true causal ordering is unknown. In fact, we need to consider the set of all causal orderings for each DAG in the MEC of the true DAG. Thus we first learn the CPDAG of the true underlying DAG using the PC algorithm and then apply Algorithms \ref{alg:dagtocstree} and \ref{alg:learncstree}.

#+begin_export latex
\begin{algorithm}\label{alg:cstreepc}
\SetAlgoLined
\KwIn{Data matrix $D$, (optional causal ordering $O$)}
\KwOut{List of CStrees $T$ with minimum number of stages}
$CPDAG \gets$ \textsc{PcAlgorithm}($D$)\;
\uIf{$O$ given}
{
$G \gets g \in CPDAG$ with ordering $O$\;
$dags \gets$ [$G$]\;
$orderings \gets [O]$\;
}
\uElse{
$dags \gets $ [$g$ in CPDAG]\;
}
$min\_stage\_trees \gets []$\;
$min\_stage \gets \infty$\;
\For{$G$ in $dags$}{


\If{$O$ not given}{
    $orderings \gets$  \textsc{AllTopologicalSort}($G$)\;}

    \For{$O$ in $orderings$}{
    $S,T \gets $ \textsc{DagToCStree($G$,$O$)}\;
    $S,T \gets $ \textsc{LearnCStree($T,S,O,D$)}\;
    \If{$|S|$ < $min\_stages$}
    {
    $min\_stages \gets$ |S|\;
    $min\_stage\_trees \gets$ [($T,S$)]\;
    }
    \If {$|S| = min\_stages$}{
    $min\_stage\_trees.append((T,S))$\;}

    

}
}
\KwRet{$min\_stage\_trees$}
\caption{\textsc{CStreePcAlgorithm} \\ Learning a CStree from observational data}

\end{algorithm}
#+end_export


We consider all topological orderings of all Markov equiavalent DAGs learnt by the PC algorithm because we might not be able to encode some context specific information otherwise. We show an example of this in the next section after introducing minimal context DAGs.

Algorithm \ref{alg:cstreepc} considers many possible candidate CStree models, thus we have to pick the best model with respect to some criterion. There are however many instances where we could know the causal ordering apriori cite:thwaites-2010-causal-analy,silander-2013, for example a temporal relation between nodes known through physical laws. In this case we can either start statistical testing from an empty tree, or apply the PC algorithm to the data and find a DAG in the Markov Equivalence class with the known ordering and then run the extra testing.



Unlike DAGs, as the number of variables increase it gets progessively harder to visually understand the learnt causal structure by just looking at the learnt CStree. 
    
# !!! ** TODO Learning CStrees from internventional data

** Understanding high-dimensional CStrees
   From a pragmatic perspective the aim of this section is to introduce the notion of Minimal Context (MC) DAGs which can help visualize CStrees with more variables and the context specific information they encode. On a theoretical note, this work has led to
  the generalization of Theorem \ref{thm:vermapearl} to define a characterization of Markov Equivalence for CStrees cite:duarte-2021-repres-learn. We start by first describing the procedure {{{footnote(\textsc{StagesInLevel} takes a set of stages and a level and returns the stages in that level; \textsc{ContextOfStage} takes a stage and returns the common context of that stage; \textsc{VariablesOfContext} takes a context and returns the variables in it)}}} to generate the CSI relations from a CStree and its stages, which uses Lemma \ref{lem:cstreecsi}
   #+begin_export latex

   \begin{algorithm}\label{alg:gencsirels}
  \SetAlgoLined
  \KwIn{CStree $T$, its stages $S$ and its causal ordering $O$}
  \KwOut{Set of CSI Relations $J$ encoded in the CStree}
  $l=1$\;
  $p=|O|$\;
  $J = []$\;
  \While{$l<p$}{
  $S_l \gets $ \textsc{StagesInLevel($S,l$)}\;
  \For{$s$ in $S_l$}{
  $c \gets $ \textsc{ContextOfStage}($S$)\;
  $v_c \gets $ \textsc{VariablesOfContext}($c$)\;
  $v_o \gets O[1:l-1] \textbackslash v_c$\;
  $J.append((X_{O[l+1]} \indep X_{v_o} \, | \, c))$
  % \tcp{Note here $c$ is a variable representing a context}
     }
  }
  
\caption{\textsc{GenerateCsiRelations} \\ Generate the CSI Relations from the CStree}

   \end{algorithm}


\begin{theorem}\label{thm:gencsirelscorrectness}
Given a CStree $\mathbb{T}$, Algorithm \ref{alg:gencsirels} is correct and returns the CSI relations encoded in $\mathbb{T}$ in $\mathcal{O}(pd^{2p})$ time.
\end{theorem}
\textit{Proof: Correctness follows directly from Lemma \ref{lem:cstreecsi} since at each loop we add exactly the CSI relations mentioned inthe lemma, and we do this for all levels thus include all stages of the CStree. For time complexity, for each level we first get the stages associated with it, which can be done in constant time if we store this information. We know from the Proof of Theorem \ref{thm:dagtocstreecorrectness} that the number of stages in level $i$ is bounded above by $d^i$, and for each stage in that level we get the context of the stage, which can be done as a constant lookup operation, and we get relevant variables in Lines 8,9 which is bounded above by $2p$. Thus adding this for all the levels give $\sum_{i=1}^p 2pd^i = \mathcal{O}(pd^{2p})$.
}
   #+end_export
 

   In practice a slightly modified version of Algorithm \ref{alg:gencsirels} can be placed a subroutine in the previous algorithms right before moving onto the next level.


   From Lemma \ref{lem:cstreecsi} we know any distribution in the CStree model $\mathbb{M}_{(\mathbb{T},\theta)}$ encodes the CSI relations of the given form, there could be more CSI relations satisfied by every distribution in $\mathbb{M}_{(\mathbb{T},\theta)}$, which is similar to how a DAG model encodes the CI relations $\mathbb{J}_1$ implied by the local Markov property, which are captured by the CI relations $\mathbb{J}_2$ from the global Markov property (i.e. from the d-separations), and $\mathbb{J}_1 \subset \mathbb{J}_2$.


 The complete of set of all CSI relations satisfied by each distribution $\mathbb{P} \in \mathbb{M}_{(\mathbb{T},\theta)}$ includes the CSI relations recovered from Algorithm \ref{alg:gencsirels}, and also include those implied by the succesive application of the Context Specific Conditional Independence axioms to generate further CSI relations. The axioms are as follows

   
   1. Symmetry, If $(X_A \indep X_B \,|\, X_C=x_c) \in \mathbb{J}$ then $(X_B \indep X_A \,|\, X_c=x_c) \in \mathbb{J}$ 
   2. Decomposition, If $(X_A \indep X_{B \cup D} \,|\, X_S, X_C=x_c) \in \mathbb{J}$ then $(X_A \indep X_B \,|\, X_S, X_C=x_c) \in \mathbb{J}$
   3. Weak union, If $(X_A \indep X_{B \cup D} \,|\, X_S, X_C=x_c) \in \mathbb{J}$ then $(X_A \indep X_{B} \,|\, X_{S \cup D}, X_C=x_c) \in \mathbb{J}$
   4. Contraction, If $(X_A \indep X_B \,|\, X_{S \cup D}, X_C=x_c) \in \mathbb{J}$ and $(X_A \indep X_D \,|\,X_S, X_C=x_c) \in \mathbb{J}$ then $(X_A \indep X_{B \cup D} \,|\, X_S, X_C=x_c) \in \mathbb{J}$
   5. Intersection,  If $(X_A \indep X_B \,|\, X_{S \cup D}, X_C=x_c) \in \mathbb{J}$ and  $(X_A \indep X_B \,|\, X_{B \cup D}, X_C=x_c) \in \mathbb{J}$ then  $(X_A \indep X_{B \cup S} \,|\, X_D, X_C=x_c) \in \mathbb{J}$
   6. Specialization, If $(X_A \indep X_B \,|\, X_S, X_C=x_c) \in \mathbb{J}$ and $T \subset S, x_T \in \mathcal{X}_T$ then $(X_A \indep X_B \,|\, X_{S \textbackslash T}, X_{T \cup C} = x_{T \cup C}) \in \mathbb{J}$
   7. Absorption, If $(X_A \indep X_B \,|\, X_S, X_C=x_c) \in \mathbb{J}$ and $\exists T \subset C$ such that $\forall x_T \in \mathcal{X}_T$ we have $(X_A \indep X_B \,|\, X_S, X_{C\textbackslash T}=x_{C \textbackslash T}, X_T=x_T) \in \mathbb{J}$ then $(X_A \indep X_B \,|\, X_{S \cup T}, X_{C \textbackslash T}=x_{C \textbackslash T}) \in \mathbb{J}$


   Given a Context Specific Conditional Independence model $\mathbb{J}$, the successive application of the axioms above results in the Context Specific closure  of the model, denoted $\mathbb{\overline{J}}$.


   The Absorption axiom helps us to get a representation of the CStree as a sequence of DAGs. For this we need the definition of minimal contexts cite:duarte-2021-repres-learn.
   #+begin_export latex
\begin{definition}[Minimal contexts]\label{def:mcs}
Given a set Context Specific Independence model $\mathbb{J}$, we say that ${X_C=x_C}$ is a minimal context if we have $(X_A  \indep X_B \,|\, X_S, X_C=x_C) \in \mathbb{J}$ and there is no non-empty subset $T \subset C$ such that $(X_A \indep X_B \,|\, X_{S \cup T}, X_{C \textbackslash T}=x_{C\textbackslash T}) \in \mathbb{J}$.
\end{definition}
   
   #+end_export
   
Intuitively, the minimal contexts are the smallest contexts that get left behind after repeated application of the Absorption axiom. By the Specialization axiom, given a minimal context $(X_C=x_C)$ we can recover all the CI relations implied by the CStree.

Minimal contexts and the application of the graphoid axioms are key to represent the CStree as a sequence of DAGs. For now, we denote $\mathbb{C}(\mathbb{T})$ as the set of all minimal contexts of a CStree $\mathbb{T}$, for each minimal context $(X_C=x_C)_i \in \mathbb{C}(\mathbb{T})$, there exists a DAG that encodes the CI relations that hold under this context.  We denote $\mathbb{G}(\mathbb{T}) := \{ \mathbb{G}_{X_C=x_C} \}_{X_C=x_C \in \mathbb{C}(\mathbb{T})}$ as the set of all minimal context DAGs of $\mathbb{T}$, and define the procedure to generate them later on.


We give some examples of minimal contexts below.

#+begin_export latex
\begin{example}\label{eg:mc1}
Let $X_1,X_2,X_3,X_4,X_5$ be binary variables taking values in $\{0,1\}$. If we have just the CSI relations $(X_5 \indep X_4 \,|\, X_{\{1,2,3\}}=000)$ and $(X_5 \indep X_4 \,|\, X_{\{1,2,3\}}=100)$ they get absorbed into $(X_5 \indep X_4 | X_{\{1\}}, X_{\{2,3\}}=00)$ leaving the minimal context $X_{\{2,3\}}=00$, or simply $(X_2 = 0, X_3=0)$.
\end{example}

\begin{example}\label{eg:mc2}
Let $X_1,X_2,X_3,X_4$ be binary variables taking values in $\{0,1\}$. If we have the  CSI relations $(X_4 \indep X_2 \,|\, X_{\{1,3\}}=00), $\\$ (X_4 \indep X_2 \,|\, X_{\{1,3\}}=01), (X_4 \indep X_2 \,|\, X_{\{1,3\}}=10), (X_3 \indep X_1 \,|\, X_2=1)$ then they absorb to give the equivalent CSI relations $(X_4 \indep X_2 \, |\,X_1, X_3=0), (X_4 \indep X_2 \,|\, X_3, X_1=0), (X_3 \indep X_1 \,|\,X_2=0)$ thus the minimal contexts are $(X_1=0),(X_2=0),(X_3=0)$
\end{example}

#+end_export
# TODO add cstree of example eg:mc2 as margin figure

Example \ref{eg:mc1} shows that we can get absorption from CSI relations we get from different levels, while \ref{eg:mc2} shows that we have to check at least all possible pairs of CSI relations to get the minimal contexts. It is also possible to get the empty context as the minimal context, which happens when the all CSI relations $(X_A \indep X_B \,|\, X_S, X_C=x_C)$ hold for all outcomes $x_C \in \mathcal{X}_C$ in which case the absorption axiom gives the CI relation $(X_A \indep X_A\,|\,X_S,X_C)$.


From a computational perspective, given all CSI relations involving sets of variables $X_A,X_B$ i.e. those of the form $(X_A \indep X_B \,|\, X_S, X_C=x_C)$, we want to find the largest set $T \subset C$ such $(X_A \indep X_B \,|\, X_S, X_T=x_T, x_{C \textbackslash T} = x_{C \textbackslash T})$ is also in the CSI relations. {{{footnote(If we are to use the CSI relations extracted from Algorithm \ref{alg:gencsirels} before getting their Context Specific closure they would be of the form $(X_i \indep X_B | X_C=x_C)$ where $i \in [p]$ and $B \,C \subset [p-1]$)}}} This can be thought of as decomposing $C$ into sets $C \textbackslash T$ and $T$ such that Definition \ref{def:mcs} holds. To find the largest such $T$, we can perform a binary search on the size of $T$, which can range from $0$ to $|C|$.
We describe this method below. {{{footnote(Each break statement leaves the first loop it meets.)}}}
#+begin_export latex
\begin{algorithm}\label{alg:genmcs}
  \SetKw{Break}{break}
  \SetAlgoLined
  \SetKwBlock{Begin}{Begin}{}
  \KwIn{Set of CSI Relations $J$}
  \KwOut{Set of minimal contexts $MCs$ from $J$}
  $set\_pairs \gets$ \textsc{PairsOfSets}($J$),$MCs \gets$ \textsc{EmptyDictionary}\;
  \For{$A,B$ in $set\_pairs$}{
    $rels \gets$ \textsc{RelsOfPair}($A,B$),$cs \gets$ \textsc{ContextsOfRels}($rels$)\;
    \For{$rel$ in $rels$}{
        $C \gets$ \textsc{VariablesOfContext}(\textsc{ContextOfRels}($rel$))\;
        $T\_sizes \gets$ [0,...,|C|],$T\_found=False$\;
        \While{not $T\_found$}{
            $mid \gets |T\_sizes|$/2 , $T\_size \gets T\_sizes[mid]$\;
            $T\_candidates \gets$ [subsets of $C$ of size $T\_size$] \;
            \For {$T$ in $T\_candidates$}{
                $T\_contained \gets False$\;
                $x_{Ts} \gets$ \textsc{CartesianProduct}($T$),
                $x_{Ts}\_count = 0$\;
                \For {$x_T$ in $x_{Ts}$} {
                    $x_{T}\_contained \gets$ True if $x_T$ in $cs$ else False\;
                    \If{$x_{T}\_contained$ is True}{
                        $x_{Ts}\_counts +=1$\;
                        \If{$x_{Ts}\_counts = |x_{Ts}|$}{
                             $T\_contained \gets$ True\;
                         $search\_upperhalf \gets$ True\;
                             }
                       }
                    \If{$x_T\_contained$ is False}{
                    $search\_lowerhalf \gets$ True
                    \Break}

                }
                \If{(T\_contained and |T\_candidates|=1) or T\_sizes=[0] or T\_sizes=[|C|]}{
                    $T\_found \gets True$\;
		$MCs \gets$ \textsc{UpdateMinContexts}($MCs,rels$)\;
                }


                \uIf{search\_upperhalf}
                {T\_sizes = T\_sizes[mid:end]
                \Break}
                \If{search\_lowerhalf}
                {T\_sizes = T\sizes[start:mid]
                \Break}

            }


            \If{T\_found}
            {\Break}
        }
        \If{T\_found}
        {\Break}
    }}
  }\caption{\textsc{GenerateMinimalContexts} \\ Generate the Minimal Contexts from a set of CSI relations}

\end{algorithm}
#+end_export

#+begin_export latex
\begin{theorem}\label{alg:genmcscorrectness}
Given a set of Context Specific Independence relations $\mathbb{J}$, Algorithm \ref{alg:genmcs} returns the set of minimal contexts in [!!! a long time theoretically]
\end{theorem}
#+end_export

[!!! Describe subprocedures in minimal context generator algorithm]

In theory, the number of possible pairs of sets $A,B$ is ${2^p}\choose{2}$, though in practice it can be reduced. For example by the Symmetry axiom allows us to half the number of pairs we need to check. This motivates us to the pairwise case, where we focus on Context Specific Independence relations of the form between 2 variables $X_i,X_j$ rather than two sets of variables $X_A,X_B$. This drops the number of possible pairs to $p \choose 2$. This leads us to the definition of pairwise minimal contexts.

{{{NEWPAGE}}}
   #+begin_export latex
\begin{definition}[Pairwise minimal contexts]\label{def:pairmcs}
Given a set Context Specific Independence model $\mathbb{J}$, we say that ${X_C=x_C}$ is a pairwise minimal context if we have $(X_i  \indep X_j \,|\, X_S, X_C=x_C) \in \mathbb{J}$ and there is no non-empty subset $T \subset C$ such that $(X_i \indep X_j \,|\, X_{S \cup T}, X_C=x_C) \in \mathbb{J}$.
\end{definition}
   #+end_export

We can make use of the Context Specific Conditional independence axioms to generate pairwise CSI relations to get the pairwise minimal contexts.


The introduction of pairwise relations motivates the inclusion of the following axiom.
8. [@8] Composition, If $(X_A \indep X_B \,|\, X_S, X_C=x_C)$ and $(X_A \indep X_D \,|\,X_S,X_C=x_C)$ then $(X_A \indep X_{B\cup D} \,|\,X_S,X_C=x_C)$

   
The composition axiom allows us to go from pairwise relations between variables to the more general pairwise relations between sets of variables. We call a Context Specific Conditional Independence model satisfying the additional composition axiom a Context Specific compostional graphoid, which generalizes the notion of compositional graphoids cite:sadeghi-2014-markov-proper. An important question is whether the Context Specific closure of CSI relations from a CStree form a compositional context specific graphoid.


Pairwise minimal contexts are always minimal contexts however but we could have minimal contexts that are not pairwise minimal contexts. If all CStrees have an associated compositional context specific independence model, the pairwise relations may indeed be all that we need to get the set of complete set of minimal contexts. We leave this as an open question and use pairwise minimal contexts to offer a visualization of CSI relations in the CStree which may potentially be incomplete.



We now have the machinery to visualize higher dimensional CStrees. We start by generating the CSI relations from the trees. Then get the Context Specific closure, followed by the minimal contexts. Once we have the minimal contexts, the CI relations for each minimal context define a graphoid. This is called a Minimal Context DAG. Thus, the CStree can be represented as a sequence of DAGs for each minimal context. This representation is a consequence of the following theorem cite:duarte-2021-repres-learn.

{{{NEWPAGE}}}

#+begin_export latex
\begin{theorem}[Markov theorem for CStrees]\label{thm:markovtheoremcstrees}
Given a CStree $\mathbb{T}$, with levels $L_1,...,L_p \sim X_1,...,X_p$, minimal contexts $\mathbb{C}(\mathbb{T})$ and $\mathbb{P} \in \Delta_{|\mathcal{X}|-1}$. The following are equivalent.
\begin{itemize}
\item $\mathbb{P}$ factorizes according to $\mathbb{T}$.
\item $\mathbb{P}$ is Markov to $\mathbb{G}(\mathbb{T})$.
\item $\forall \, X_C = x_C \in \mathbb{C}(\mathbb{T})$ we have\\ $\mathbb{P}(X_{[p]\textbackslash C}\,|\,X_C=x_C) = \prod_{k \in [p]\textbackslash C} \mathbb{P}(X_k \, |\, X_{PA_{\mathbb{G}_{X_C=x_C}}(k)}, X_C=x_C)$.
\end{itemize}
\end{theorem}
#+end_export


This DAG is also called the minimal I-MAP cite:verma-1990-causal-networ, which we can recover with the following procedure cite:solus-2021-consis-guaran.


   #+begin_export latex
\begin{algorithm}\label{alg:mcdags}
\SetAlgoLined
\KwIn{Causal ordering $O$, Minimal Contexts $MCs$ as a dictionary with minimal contexts as keys and the CI relations under the minimal context as values}
\KwOut{List of minimal contexts with their minimal context DAGs $MCDAGS$}
$MCDAGS \gets []$\;
\For{$MC$, $Ci\_Rels$ in $MCs$}{
$G \gets$ Empty Graph\;
    $nodes \gets O \textbackslash$ \textsc{VariablesOfContext}($MC$)\;
    \For{$i$ in [1,...,|$nodes$|]}{
        \For{$j$ in [$i+1$,...,|$nodes$|]}{
	    $\pi_i \gets nodes[i]$\;
	    $\pi_j \gets nodes[j]$\;
	    $G.add\_edge(\pi_i, \pi_j)$\;
	    $S = O[1:j-1] \textbackslash $\textsc{VariablesOfContext}($MC$) \textbackslash $\{\pi_i\}$ \;

	    \If{$(X_{\pi_i} \indep X_{\pi_j} \,|\, X_S) \in Ci\_Rels$}{
	        $G.remove\_edge(\pi_i,\pi_j)$
	    }
	}
    }


$MCDAGS.add((MC,G))$\;
}

\caption{\textsc{GenerateMinContextDags} \\ Generating minimal context DAGs}
\KwRet{$MCDAGS$}
\end{algorithm}
   #+end_export

   # !!! TODO  Can we make use of any ordered property to speed up the search to check if we can remove edge in IMAP above

   # [!!! TODO Lemma on why just checking if there is a ci rel with a conditioning set being a subset of $X_1,...,X_{j-1}\textbackslash X_i\textbackslash C$  works]


#+begin_export latex
\begin{theorem}\label{thm:mcdagscorrectness}
Given variables $X_1,...,X_p$, a set of minimal contexts $C$ and for each minimal context $C$ the conditional independence relations $\mathbb{J}_C$ that hold under this minimal context, Algorithm \ref{alg:mcdags} is correct and runs in $\mathcal{O}(p^2 |C||\mathbb{J}|)$ time.
\end{theorem}


%\textit{Proof:
%}
#+end_export
# !!! TODO Proof of above



We end this section by stating the definition of faithfulness for CStrees which allows us to explain why we consider all Markov equivalent DAGs when learning a CStree, followed by the main theorem for Algorithm \ref{alg:cstreepc}

#+begin_export latex
\begin{definition}[Faithfulness for CStrees]\label{def:faithfulnesscstrees}
A distribution $\mathbb{P}$ is faithful to a CStree $\mathbb{T}$ if it entails exactly the CSI relations encoded by the set of minimal context DAGs $\mathbb{G}(\mathbb{T})$.
\end{definition}
#+end_export

To see why we need to consider all Markov equivalent DAGs in Algorithm \ref{alg:cstreepc}, suppose we have the following case with 4 binary variables.


 #+begin_export latex
\begin{figure}[!h]\label{fig:dagtocstree_cstree}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1\linewidth]{figures/exampleonallmec.pdf}}%
\caption{Example on why to consider all topological ordering of all Markov equivalent DAGs when learning a CStree}
        
   \end{floatrow}
\end{figure}
   #+end_export

   
   Let $\mathbb{P}$ be the data generating distribution and suppose it is faithful to the CStree with the context graph shown above for the context $X_4=0$. The empty context DAG could possibly be the fully connected shown above. If we do happen to learn this DAG from the PC algorithm step, it only has one topological ordering which is 4321, and there is no way to join the stages for the empty context graph which this order to encode $(X_1 \indep X_3 \,|\,X_4=0)$, in which case we do not learn the true model.
   

#+begin_export latex
\begin{theorem}\label{thm:cstreepccorrectness}
Given variables $X_1,...,X_p$, assuming that the data generating distribution is faithful to some unknown CStree $\mathbb{T}$ with a known causal ordering $\pi = \pi_1 \cdots \pi_p$, Algorithm \ref{alg:cstreepc} is consistent, i.e. it recovers $\mathbb{T}$ as the number of samples $n \rightarrow \infty$.
\end{theorem}
#+end_export
# !!! TODO Proof of above


** Model selection for CStrees
   In Algorithm \ref{alg:cstreepc} we select the CStree (or CStrees if there is more than one) with the causal ordering corresponding to the fewest stages, which is motivated by the principle of Occams razor cite:pearl-2009-causal since few stages imply lower model complexity and all else being, the simplest model is the best model. The original authors also present the *Bayesian Information Criterion (BIC)* for CStrees alongside a proof that it is locally consistent for CStrees, meaning it can be applicable to greedy search methods for CStrees. The BIC depends on two terms, the likelihood of the data, which assesses the quality of the model to explain the observed data, and a complexity term that depends on the amount of observed data and the free parameters of the model, with the idea being that models with more parameters should be penalized. Importantly, this helps against overfitting since one can simply add more parameters to the model to maximize the likelihood.


For a general model with random variables $X_1,...,X_p$ the likelihood of observing a sample $(x_1,...,x_p)$ can be described as below {{{footnote(Here we compactify the notation further - $X_{\{1\,...\,p\}}=x_{\{1\,...\,p\}}$ is the same as $X_{\{1\,...\,p\}}=x_1\cdots x_p$ which is $X_1=x_1\,...\,X_p=x_p$ )}}}
.


#+begin_export latex
\begin{align*}
\mathbb{P}(X_{\{1,...,p\}}=x_{\{1\cdots x_p\}}) =\mathbb{P}(X_i=x_i|X_{\{i-1,...,1\}}=x_{\{i-1,...,1 \}})\\\cdots\mathbb{P}(X_2=x_2|X_1=x_1)\mathbb{P}(X_1=x_1)
\end{align*}
#+end_export

In the CStree model, independence is implied from the staging of the tree, and each distribution in the product above depends on the fixed context of the node in the CStree representing that distribution. Denoting $C_i$ to be the context variables of the context of the node $x_{1\cdots i}$, this gives the following.

#+begin_export latex
\begin{align*}
\mathbb{P}(X_{\{1,...,p\}}=x_{\{1,..., p\}}) =\prod_{i=1}^p \mathbb{P}(X_i=x_i|X_{C_i}=x_{C_i})
\end{align*}
#+end_export


In order to get these values from the data, we denote $\mathbb{U}$ to be *contingency table* of the data, which a multi-dimensional array {{{footnote(This is the same as tensors in the context of computer science)}}}  with dimensions $d_1 \times \cdots \times d_p$, where $d_i$ is the number of possible outcomes for variable $X_i$. Given a sample $(x_1,...,x_p)$, the value in $\mathbb{U}[(x_1,...,x_p)]$ is the number of times we have this sample in the dataset. The marginalized contingency table $\mathbb{U}_C$ represents the table after summing over the axes which are not in $C$. An important case is when $C$ is the empty set, which results in the marginal table to be 1 dimensional scalar and is the total number of samples in the dataset - when having a stage with an empty context this reflects the conditional distribution for the corresponding variable to simply be the ratio of its outcomes in the dataset. This allows the following compact representation for the likelihood cite:duarte-2021-repres-learn for a tree with levels $(L_1,...,L_p) \sim (X_1,...,X_p)$.

#+begin_export latex
\begin{align*}
\mathbb{P}(X_{\{1,...,p\}}=x_{\{1,..., p\}}) = \prod_{k=1}^p \frac{\mathbb{U}_{C \cup k}[x_{C \cup k}]}{\mathbb{U}_{C}[x_{C}]}
\end{align*}
#+end_export

Given $n$ samples arranged into a $n \times p$ array $\mathbb{D}$, and under the assumption that they are independent, the total likelihood is simply the product over all samples in $\mathbb{D}$.


The free parameters is $d=\sum_{k=1}^{p-1} (|\mathcal{X}_k| - 1)S_k$ where $S_k$ is the number of distinct stages in level $k$ cite:duarte-2021-repres-learn. The  BIC score for a CStree $\mathbb{T}$ with observed data $\mathbb{D}$ is then

#+begin_export latex
\begin{align*}
\textbf{BIC}(\mathbb{T},\mathbb{D}) = \log\mathbb{P}(\mathbb{D}\,|\,\mathbb{T}) - \frac{d}{2}\log(n)
\end{align*}
#+end_export


   
# !!! TODO Example on how pairwise independence does not imply triplewise independence



* Experiments
The aim of this section is to run test the theory presented so far in both synthetic and real world data. Namely, for all the datasets we use, we ask ourselves the following.

1. Given a DAG and a causal ordering, is there a difference between learning the context specific independence relations after encoding the CI relations from the DAG into CStree, or without encoding any of these CI relations?
3. Is the CStree property assumption reasonable in practice?
4. How sensitive are the results on the method used to determine whether nodes belong to the same stage?
5. How are the results in comparison to CStrees generated from DAGs learnt using other causal discovery algorithms?
6. How well do our learnt CStrees compared to staged tree models learnt using different algorithms?

   
Throughout this section we omit the final layer of nodes for all the CStrees since they always belong to the singleton stage.

# TODO Why do they all belong to the singleton stage

  
** Encoding DAGs as CStrees
   We first run a sanity check on generating the CStrees from DAGs in the 2 extreme cases, when we have no connectivity and full connectivity.

 #+begin_export latex
\begin{figure}[!h]\label{fig:dagtocstree_cstree}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1\linewidth]{figures/fulldagtocstree.pdf}}%
\caption{Generating the CStrees for a fully connected DAG using the ordering 1234 with Algorithm \ref{alg:dagtocstree}}
        
   \end{floatrow}
\end{figure}

\begin{figure}[!h]\label{fig:dagtocstree_cstree}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1\linewidth]{figures/emptydagtocstree.pdf}}%
\caption{Generating the CStrees for DAG with no edges using the ordering 1234 with Algorithm \ref{alg:dagtocstree}}
        
   \end{floatrow}
\end{figure}
   #+end_export

The results are as expected since there are no conditional independence relations encoded in the fully connected model, and no context specific independence statements since they are unable to encode them, meaning no 2 nodes share a fixed context and all stages have a single element. For the DAG with no edges, each variable is independent of all others, and each node has no parents. Thus by Proposition \ref{prop:dagsandcstrees} all levels share the empty context as the common context. 
  
** Recovering the empty context DAG
   We start by generating random DAGs and generating the CStrees as per Algorithm \ref{alg:dagtocstree}. This is done by first choosing the number of variables (nodes in the DAG) $p$, and then choosing an edge with probability $p_{edge} \in (0,1)$, and then keeping edge $(u,v)$ if and only if $u<v$. The causal ordering is chosen as $12\cdots p$. We show the generated CStree below, and recover the original DAG as a minimal context DAG with the empty context with Algorithm \ref{alg:mcdags}. 

  #+begin_export latex
\begin{figure}[!h]\label{fig:dagtocstree_cstree}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1.1\linewidth]{figures/dagtocstreetoemptycontextdag.pdf}}%
\caption{Generating CStrees from random DAGs and recovering the original random DAG from the CStree with ordering 1234 using Algorithm \ref{alg:mcdags}}
        
   \end{floatrow}
\end{figure}
   #+end_export

   
#+BEGIN_COMMENT
We show a CStree for a random DAG model with 8 binary variables below in a more compact visualization, alongside the recovered empty context DAG in Figures \ref{fig:dagtocstree_cstree8}\ref{fig:dagtocstree_dag8}.

#+begin_export latex
\begin{figure}[H]
\label{fig:dagtocstree_cstree8}
\includegraphics[\linewidth]{figures/dagtocstree_cstree8.pdf}
\caption{CStree corresponding to the 8 node DAG in Figure \ref{fig:dagtocstree_dag8}}
\end{figure}

\begin{figure}
\centering
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.45\linewidth]{figures/dagtocstree_dag8.pdf}
  \caption{A subfigure}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{\textwidth}
  \centering
  \includegraphics[width=.45\linewidth]{figures/dagtocstree_mcdag8.pdf}
  \caption{A subfigure}
  \label{fig:sub2}
\end{subfigure}
\caption{Left: A random DAG with 10 binary variables which represents the CStree in Figure \ref{fig:dagtocstree_cstree8}. Right: The corresponding empty context DAG from that CStree}
\label{fig:dagtocstree_dag8}
\end{figure}



#+end_export
#+END_COMMENT

The following table summarizes the empirical space time complexity of generating a CStree from certain random DAGs generated from the aforementioned procedure.
   
 #+CAPTION: Statistics from generating CStrees for random DAGs, Space refers to the amount of RAM occuppied by the CStree structure, and time refers to the amount of time taken to encode the CI relations in the DAG to the CStree. 
 |------------+-----------+-------------+-----------|
 |          / |         < |           < |         < |
 | $p_{edge}$ | DAG Nodes | Space  (GB) | Time  (s) |
 |------------+-----------+-------------+-----------|
 |        0.2 |        20 |       1.203 |        14 |
 |        0.2 |        21 |       2.422 |        43 |
 |        0.2 |        22 |       4.875 |        74 |
 |        0.2 |        23 |       9.812 |       240 |
 


 # !!! TODO Get the average of above for some number of runs

** Synthetic data
    Next we generate a random DAG $\mathbb{G}$ using the procedure above, and then generate samples from this DAG, where each variable is binary valued. We then discard the DAG and apply Algorithm \ref{alg:cstreepc} on the generated samples. For nodes which are adjacent to another node, we sample the data by constructing a conditional probability distribution table by taking the parents of the nodes, and creating one row for each possible outcome of the parents. In this binary case, if a variable $X_i$ has $d$ parents $X_{PA_{X_1}},...,X_{PA_{X_d}}$, the corresponding table has $2^d$ rows. The 2 columns represent the outcome 0 and 1 respectively. The probability values are filled by taking a row corresponding to some outcome of the parents, and assigning the probability of $X_i=0$ under this outcome to be distributed uniformly between $[0.01,0.2]$ or $[0.8,0.99]$ each with a probability 0.5. The probability for $X_i=1$ under this outcome is then simply computed such that the probabilities sum to 1. For the variables corresponding to nodes with no edges, we give them a Bernoulli distribution with a parameter chosen uniformly between $[0,1]$ for each such variable.


    
    We start by generating 1000 samples on 4 binary variables using the above procedure. We then learn the CStree and recover the minimal context DAGs. We choose 4 variables due to limitations of applying the context specific graphoid axioms.

    The DAG and conditional probability table is as follows.

    # TODO Put DAG of this example

    
  #+begin_export latex
\begin{figure}[!h]\label{fig:synthetic_exp1}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1.1\linewidth]{temp/eppswdagnotremovingskewed.pdf}}%
\caption{Generating CStrees from datasets generated by random DAGs and learning the context specific causal structure using Algorithm \ref{alg:cstreepc} and \ref{alg:mcdags}}
        
   \end{floatrow}
\end{figure}
   #+end_export


   The above procedure could produce non empty minimal contexts for several reasons, for example due to small sample sizes, or errors in the context specific independence testing procedure.

   

   


   In order to assess the influence of the CI relations from the DAG in the final CStree, we tested the use of Algorithm \ref{alg:cstreepc} with the following experiment. First we generate a dataset from the above procedure, then we run Algorithm \ref{alg:cstreepc} on this dataset with and without using the CI relations from the CPDAG learnt in the first step. Below is an example where the DAG picks up a conditional independence relation which is not picked up by strictly running only the context specific independence tests.

   
     #+begin_export latex
\begin{figure}[]\label{fig:synthetic_exp2a}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1.1\linewidth]{temp/andersonwdag_subsample.pdf}}%
\caption{CStree learnt using the CI relations from the DAG, which encodes the relation $X_3 \indep X_2,X_1$}
        
   \end{floatrow}
\end{figure}
\begin{figure}[]\label{fig:synthetic_exp2b}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1.1\linewidth]{temp/andersonwoutdag_subsample.pdf}}%
\caption{CStree learnt without the CI relations from the DAG, which does not encode the relation $X_3 \indep X_2,X_1$ unlike when using the DAG on the same dataset}}
        
   \end{floatrow}
\end{figure}
   #+end_export

{{{NEWPAGE}}}
** Coronary disease data
  
   This is a dataset consisting of features which might increase the risk for coronary thrombosis, and consists of 1841 samples from men cite:reinis-1981-progn-signif.  These features are detailed below.
 |---------------------------------------+------+-----------------------------------|
 | /                                     |    < | <                                 |
 | Variable name                         | Node | Outcomes                          |
 |---------------------------------------+------+-----------------------------------|
 | Smoking                               |    1 | $\{ Yes,No \}$                    |
 | Strenuous mental work                 |    2 | $\{ Yes,No \}$                    |
 | Strenuous physical work               |    3 | $\{ Yes,No \}$                    |
 | Systolic blood pressure               |    4 | $\{(-\infty,140],(140,-\infty)\}$ |
 | Ratio of beta and alpha lipoproteins  |    5 | $\{(-\infty,3],(3,-\infty)\}$     |
 | Coronary heart disease family history |    6 | $\{ Yes,No \}$                    |
 
                                                                     
This data set does not require further pre-processing since the outcome space is already categorical, and there were no missing values.


We first perform the following experiment: Learn a DAG model from both the PC algorithm and the Hill climbing search algorithm. Now for each topological ordering of all the DAGs which are Markov equivalent, we store the following CStrees:
1. The CStree generated from the DAG itself
2. The CStree learnt only from context specific information without encoding the CI relations from the DAG
3. The CStree learnt from encoding the CI relations from the DAG and then learning further context specific information

For each of the cases above we create/merge stages using the Anderson-Darling test cite:scholz-1987-k-sampl, the Epps-Singleton test cite:epps-1986-omnib-test, and the symmetric KL divergence with different threshold values. The use of the symmetric KL divergence is motivated by the unreliability of conditional independence testing in practice. Given 2 distributions $\mathbb{P}, \mathbb{Q}$ over the same discrete state space $\mathcal{X}$, the symmetric KL divergence is as follows. 


#+begin_export latex
\begin{align*}
D_{SKL}(\mathbb{P} \, , \, \mathbb{Q}) = \sum_{x \in \mathcal{X}} \mathbb{P}(X=x)\log \frac{\mathbb{P}(X = x)}{\mathbb{Q}(X=x)} + \mathbb{Q}(X=x)\log \frac{\mathbb{Q}(X = x)}{\mathbb{P}(X=x)}
\end{align*}
#+end_export


We apply Algorithm \ref{alg:cstreepc} to the setup above, and record the CStrees with the minimum number of stages over all possible causal orderings, alongside their BIC scores. If we have CStrees with the same number of minimum stages, we record the highest BIC score among them. We call this *experimental setup 1*, and the result are summarized in the Table \ref{table:coronary1}.


    Here we see that the minimum number of stages over all the configurations is 6, and the result CStree is shown in Figure \ref{fig:coronary1}. The causal ordering is 346125, and we get this CStree by learning the original DAG using the PC algorithm, followed by the Anderson-Darling test to learn context specific information without encoding the CI relations from the original DAG. However it can be seen that the BIC score is lower compared to the CStree generated from the DAG itself and the CStree learnt using the CI relations in the DAG before learning the context specific information. The latter CStree also has more stages than the CStree learnt without using the CI relations, which can be explained by the possibility that the DAG encoded CI relation(s) which put nodes in the same stage, which otherwise may have been put in the same stage from the context specific testing procedure. Among the trees with the lowest stages, the CStree with the highest BIC score is generated with the ordering 534126, using the symmetric KL divergence with a threshold of $5e^{-5}$ for the staging procedure, from the DAG learnt from the Hill Climbing algorithm and without encoding the CI relations from it. It shown in Figure \ref{fig:coronary2} We see that this CStree has 27 stages, which motivates us to define the *experimental setup 2*, where we aim to find the CStree with the highest BIC score instead of the minimum number of stages as in the experimental setup 1. We show the results of this in Table \ref{table:coronary2}.


We see that the results are nearly identical, other than for 2 entries corresponding to the Epps-Singleton test and symmetric KL divergence with threshold $5^{-5}$ which have an improved BIC score for the CStree learnt without the CI relations from the DAG. This could be due to the dataset allowing for few consistent causal orderings.


#+begin_export latex

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[]\label{table:coronary1}
\begin{tabular}{cccccccccccccccc}
\cline{3-12}
                        & \multicolumn{1}{c|}{\multirow{\begin{tabular}[c]{@{}c@{}}Goal:\\ Min stages\end{tabular}}} & \multicolumn{2}{c|}{Anderson}                                  & \multicolumn{2}{c|}{Epps}                                  & \multicolumn{2}{c|}{SKL $5 \times 10^{-5}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-6}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-7}$}                                     &  &  &  &  \\ \cline{3-12}
                        & \multicolumn{1}{c|}{}                                                                            & \multicolumn{1}{c|}{Stages}     & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              &  &  &  &  \\ \cline{3-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{18}         & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{PC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{\textbf{6}} & \multicolumn{1}{c|}{-6783.7} & \multicolumn{1}{c|}{13}     & \multicolumn{1}{c|}{-6812.1} & \multicolumn{1}{c|}{23}     & \multicolumn{1}{c|}{-6757.1} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-7032.3} & \multicolumn{1}{c|}{44}     & \multicolumn{1}{c|}{-6753.4}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{8}          & \multicolumn{1}{c|}{-6771.3} & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-6780.4} & \multicolumn{1}{c|}{15}     & \multicolumn{1}{c|}{-6775.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4}          &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{31}         & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{HC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{8}          & \multicolumn{1}{c|}{-6786.5} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6837.3} & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-6799.0} & \multicolumn{1}{c|}{39}     & \multicolumn{1}{c|}{-6828.0} & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-6755.9}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{8}          & \multicolumn{1}{c|}{-6788.7} & \multicolumn{1}{c|}{15}     & \multicolumn{1}{c|}{-6809.7} & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-6801.4} & \multicolumn{1}{c|}{23}     & \multicolumn{1}{c|}{-6805.4} & \multicolumn{1}{c|}{30}     & \multicolumn{1}{c|}{\textbf{-6729.5}} &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  & 

\caption{Experimental setup 1 on the coronary dataset. Each cell under the column "Stages" is the minimum number of stages amongst all causal orderings generated from the either the PC or Hill Climbing algorithm, for different staging criteria. Many orderings produce CStrees with the same number of stages, in which case the BIC score in the table above is the maximum over those CStrees.}
\end{tabular}
\end{table*}

#+end_export

#+begin_export latex
\begin{figure}[]\label{fig:coronary1}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1.1\linewidth]{temp/coronaryminstages1.pdf}}%
\caption{CStree for the coronary dataset with the lowest number of stages, which has ordering 346125.}
        
   \end{floatrow}
\end{figure}

\begin{figure}[]\label{fig:coronary2}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1.1\linewidth]{temp/coronaryminstages1topbic.pdf}}%
\caption{CStree with the highest BIC score among all CStrees with the minimum number of stages in experimental setup 1, which has ordering 534126.}
        
   \end{floatrow}
\end{figure}
#+end_export



#+begin_export latex
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[]\label{table:coronary2}
\begin{tabular}{cccccccccccccccc}
\cline{3-12}
                        & \multicolumn{1}{c|}{\multirow{\begin{tabular}[c]{@{}c@{}}Goal:\\ Min stages\end{tabular}}} & \multicolumn{2}{c|}{Anderson}                                  & \multicolumn{2}{c|}{Epps}                                  & \multicolumn{2}{c|}{SKL $5 \times 10^{-5}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-6}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-7}$}                                     &  &  &  &  \\ \cline{3-12}
                        & \multicolumn{1}{c|}{}                                                                            & \multicolumn{1}{c|}{Stages}     & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              &  &  &  &  \\ \cline{3-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{18}         & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{PC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{\textbf{7}} & \multicolumn{1}{c|}{-6783.7} & \multicolumn{1}{c|}{13}     & \multicolumn{1}{c|}{-6812.1} & \multicolumn{1}{c|}{23}     & \multicolumn{1}{c|}{-6757.1} & \multicolumn{1}{c|}{43}     & \multicolumn{1}{c|}{-6749.6} & \multicolumn{1}{c|}{44}     & \multicolumn{1}{c|}{-6753.4}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{8}          & \multicolumn{1}{c|}{-6771.3} & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-6780.4} & \multicolumn{1}{c|}{15}     & \multicolumn{1}{c|}{-6775.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4} & \multicolumn{1}{c|}{18}     & \multicolumn{1}{c|}{-6739.4}          &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{31}         & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6733.3}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{HC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{8}          & \multicolumn{1}{c|}{-6786.5} & \multicolumn{1}{c|}{31}     & \multicolumn{1}{c|}{-6837.3} & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-6799.0} & \multicolumn{1}{c|}{39}     & \multicolumn{1}{c|}{-6828.0} & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-6755.9}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{8}          & \multicolumn{1}{c|}{-6788.7} & \multicolumn{1}{c|}{15}     & \multicolumn{1}{c|}{-6809.7} & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-6801.4} & \multicolumn{1}{c|}{23}     & \multicolumn{1}{c|}{-6805.4} & \multicolumn{1}{c|}{30}     & \multicolumn{1}{c|}{\textbf{-6729.5}} &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                                       &  &  &  & 

\caption{Experimental setup 2 on the coronary dataset, the only difference to experimental setup 1 is that we now maximize the BIC score over all possible causal orderings.}
			\end{tabular}
\end{table*}
#+end_export



In order to compare our CStrees with similar staged trees, we do the following. First take the BIC optimal CStree that we have learnt which does not correspond to a DAG model. Take the ordering of this tree, and then fit staged trees using score based learning algorithms, optimizing for the BIC score. The algorithms used to fit the staged trees are the following:
   1. Hill climbing - Start from the full independence model, then in a greedy manner, for each node, either assign it an existing stage or create a new stage depending on whichever option best increases the score. 
   2. Backward hill climbing - Similar to hill climbing except we start from a full model, i.e. one where each node belongs to a singleton stage.
   3. Backward joining - Start from the full model, and iteratively merge stages depending on whether a certain measure between them is below a threshold. We use the KL divergence with twice the threshold which gave us the BIC optimal CStree from our methods.
   4. Hierarchial clustering - Start from a full model, and iteratively cluster nodes in level $k$ to $c_k$ clusters using hierarchial clustering. We use the total variation measure alongside 2 clusters for each level.
      We call our approach to fit the CStree modified Algorithm \ref{alg:cstreepc} since the BIC optimal CStree is not necessarily learnt using the PC algorithm to get the initial DAG.

We call this *experimental setup 3*, and detail the results of it on the coronary dataset below. None of the staged trees are CStrees, and they are included in the Appendix.

   
 |-------------+---------------------------------------+---------+--------|
 | /           | <                                     |       < |      < |
 | Model       | Algorithm                             |     BIC | Stages |
 |-------------+---------------------------------------+---------+--------|
 | CStree      | Modified Algorithm \ref{alg:cstreepc} | -6729.5 |     30 |
 | Staged tree | Hill climbing                         | -6645.3 |     11 |
 | Staged tree | Backward hill climbing                | -6641.1 |     13 |
 | Staged tree | Backward joining                      | -6790.4 |     57 |
 | Staged tree | Hierarchial clustering                | -6654.7 |     10 |


 
    In order to generate the sequence of DAGs that capture the context specific causal information from this dataset, we require 4 variables for computational feasibility. We do this by first removing the variable that gives us the the best score after its removal, which in this case is the variable corresponding to systolic blood pressure. We then generate the 5 BIC optimal CSTrees arising after removing each of the remaining 5 variables.


    

 # TODO In order to see the true discrepancy between the BIC score of DAGs and CStrees, we consider another method to get a starting DAG. This is to do an exhaustive search over the space of DAGs, and choose the DAG with the highest BIC score, and take the DAGs in its MEC to generate the possible causal orderings. We call this *experimental setup 3*. This task is intractable for variables more than 5, thus we have to remove 1 variable. To decide on which variable to choose, we generate all possible DAGs after removing each variable separately, and remove the variable which results in the highest BIC score. For this dataset, this happens to be the systolic blood pressure.

# Applying Algorithm \ref{alg:cstreepc} yields 55 CStrees which have the same number of stages, which is 69. If we exclude singleton stages corresponding to the last level this is 5 stages, which results in a full independence model.

# This motivated the exploration of other CStrees with a causal ordering consistent with the CPDAG. In total the coronary dataset gave rise to 288 trees, and although much of it resulted in very sparse CStrees, i.e. those with very few stages close to the minimum stages, there were some models which were more complex than this, one of which we show below. The amount of context specific independence tests skipped due to no available data was at most around 25 per CStree.





# TODO cite table number for above

# TODO With or withot DAG encodings

# TODO Print all possible orderings
{{{NEWPAGE}}}

** Mice protein expression data
   This is a dataset with expression levels of 77 proteins, measured in the cerebral cortex of 8 classes of mice cite:higuera-2015-self-organ. There are 38 control mice and 34 trisomic mice, and for each of them 15 separate measurements were taken which gives a total of 1080 samples. The aim is to identify features that can discriminate between the 8 classes that make up all possible combinations of the following 3 binary features - Control or Trisonomy, stimulated to learn or not, injected with saline or memantine. This dataset had missing values, which is why we handle by first inspecting the missing value counts for each feature and then removing features which had 180 or more missing values. This removes the expression data corresponding to the columns $BAD_N, BCL2_N, H3AcK18_N, EGR1_N, H3MeK4_N$. This still leaves 72 features, which we further reduce by recursive feature elimination cite:guyon-2002 where we first train a linear support vector classifier on all the features and prune the least important features in a greedy manner until we arrive at the number of features we want. After this we compute the medians for each feature and assign each feature to take binary outcomes depending on whether or not the value is above or below the median.



   We choose 7 features and the predictor variable, giving a system of 8 variables. This number is chosen since this gives a maximum of 196 causal orderings which is computationally feasible, meanwhile increasing the variables to 9 and 10 increase this to 1008 and 13608 respectively. These are detailed below.

 |---------------+------+-----------------------------------------|
 | /             |    < | <                                       |
 | Variable name | Node | Outcomes                                |
 |---------------+------+-----------------------------------------|
 | $AKT_N$       |    1 | $\{(-\infty, Q^1_2],(Q^1_2, \infty) \}$ |
 | $APP_N$       |    2 | $\{(-\infty, Q^2_2],(Q^2_2, \infty) \}$ |
 | $SOD1_N$      |    3 | $\{(-\infty, Q^3_2],(Q^3_2, \infty) \}$ |
 | $NR2B_N$      |    4 | $\{(-\infty, Q^4_2],(Q^4_2, \infty) \}$ |
 | $pNUMB_N$     |    5 | $\{(-\infty, Q^5_2],(Q^5_2, \infty) \}$ |
 | $IL1B_N$      |    6 | $\{(-\infty, Q^6_2],(Q^6_2, \infty) \}$ |
 | $SYP_N$       |    7 | $\{(-\infty, Q^7_2],(Q^7_2, \infty) \}$ |
 | Class         |    8 | $\{C_i \}_{i=1}^8$                      |
                                                   

Here, $Q^i_2$ refers to the median of the variable corresponding to node $i$.

   #+begin_export latex
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[]\label{table:mice1}
\begin{tabular}{cccccccccccccccc}
\cline{3-12}
                        & \multicolumn{1}{c|}{\multirow{\begin{tabular}[c]{@{}c@{}}Goal:\\ Min Stages\end{tabular}}} & \multicolumn{2}{c|}{Anderson}                                  & \multicolumn{2}{c|}{Epps}                                  & \multicolumn{2}{c|}{SKL $5 \times 10^{-4}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-5}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-6}$}                            &  &  &  &  \\ \cline{3-12}
                        & \multicolumn{1}{c|}{}                                                                            & \multicolumn{1}{c|}{Stages}     & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     &  &  &  &  \\ \cline{3-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{154}        & \multicolumn{1}{c|}{-5615.9} & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9} & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9} & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9} & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{PC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CStree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{\textbf{7}} & \multicolumn{1}{c|}{-6555.4} & \multicolumn{1}{c|}{8}      & \multicolumn{1}{c|}{-6442.3} & \multicolumn{1}{c|}{61}     & \multicolumn{1}{c|}{-5678.9} & \multicolumn{1}{c|}{64}     & \multicolumn{1}{c|}{-5640.6} & \multicolumn{1}{c|}{78}     & \multicolumn{1}{c|}{-5587.6} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CStree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{11}         & \multicolumn{1}{c|}{-6591.4} & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-6591.4} & \multicolumn{1}{c|}{15}     & \multicolumn{1}{c|}{-5702.0} & \multicolumn{1}{c|}{68}     & \multicolumn{1}{c|}{-5705.4} & \multicolumn{1}{c|}{71}     & \multicolumn{1}{c|}{-5692.2} &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{102}        & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{HC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CStree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{13}         & \multicolumn{1}{c|}{-6373.8} & \multicolumn{1}{c|}{82}     & \multicolumn{1}{c|}{-6095.1} & \multicolumn{1}{c|}{33}     & \multicolumn{1}{c|}{-6017.8} & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-5857.2} & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-5857.2} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CStree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{12}         & \multicolumn{1}{c|}{-6404.1} & \multicolumn{1}{c|}{20}     & \multicolumn{1}{c|}{-6179.9} & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-5852.3} & \multicolumn{1}{c|}{49}     & \multicolumn{1}{c|}{-5815.0} & \multicolumn{1}{c|}{59}     & \multicolumn{1}{c|}{-5837.1} &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  &  \\
                        &                                                                                                  &                                 &                              &                             &                              &                             &                              &                             &                              &                             &                              &  &  &  & 
\caption{Experimental setup 1 on the mice protein expression dataset}
			\end{tabular}
\end{table*}
   #+end_export


   The results from Table \ref{table:mice1} show once again that the Anderson-Darling test tends to merge stages more often thus resulting in a CStree with the minimum number of stages at the expense of a worse fit to the data, as indicated by the BIC score. We show the result of experimental setup 2 on this dataset in Table \ref{table:mice2}



   #+begin_export latex
% Please add the following required packages to your document preamble:
% \usepackage{multirow}


\begin{table*}[]\label{table:mice2}
\begin{tabular}{cccccccccccccccc}
\cline{3-12}
                        & \multicolumn{1}{c|}{\multirow{\begin{tabular}[c]{@{}c@{}}Goal:\\ Max BIC\end{tabular}}} & \multicolumn{2}{c|}{Anderson}                                  & \multicolumn{2}{c|}{Epps}                                  & \multicolumn{2}{c|}{SKL $5 \times 10^{-4}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-5}$}                            & \multicolumn{2}{c|}{SKL $5 \times 10^{-6}$}                            &  &  &  &  \\ \cline{3-12}


                        & \multicolumn{1}{c|}{}                                                                         & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              &  &  &  &  \\ \cline{3-12}
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                      & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9}          & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9}          & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9}          & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9}          & \multicolumn{1}{c|}{154}    & \multicolumn{1}{c|}{-5615.9}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{PC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                  & \multicolumn{1}{c|}{15}     & \multicolumn{1}{c|}{-6542.4}          & \multicolumn{1}{c|}{45}     & \multicolumn{1}{c|}{-5804.4}          & \multicolumn{1}{c|}{113}    & \multicolumn{1}{c|}{-5474.4}          & \multicolumn{1}{c|}{115}    & \multicolumn{1}{c|}{-5513.2}          & \multicolumn{1}{c|}{133}    & \multicolumn{1}{c|}{\textbf{-5433.0}} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-6591.4}          & \multicolumn{1}{c|}{36}     & \multicolumn{1}{c|}{-5906.2}          & \multicolumn{1}{c|}{75}     & \multicolumn{1}{c|}{-5598.9}          & \multicolumn{1}{c|}{76}     & \multicolumn{1}{c|}{-5602.4}          & \multicolumn{1}{c|}{79}     & \multicolumn{1}{c|}{-5589.1}          &  &  &  &  \\ \cline{2-12}
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                      & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} & \multicolumn{1}{c|}{102}    & \multicolumn{1}{c|}{\textbf{-5376.6}} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{HC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                  & \multicolumn{1}{c|}{13}     & \multicolumn{1}{c|}{-6373.8}          & \multicolumn{1}{c|}{85}     & \multicolumn{1}{c|}{-6019.9}          & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-5857.2}          & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-5857.2}          & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-5857.2}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                & \multicolumn{1}{c|}{13}     & \multicolumn{1}{c|}{-6387.2}          & \multicolumn{1}{c|}{21}     & \multicolumn{1}{c|}{-6149.2}          & \multicolumn{1}{c|}{41}     & \multicolumn{1}{c|}{-5852.3}          & \multicolumn{1}{c|}{49}     & \multicolumn{1}{c|}{-5815.0}          & \multicolumn{1}{c|}{59}     & \multicolumn{1}{c|}{-5837.1}          &  &  &  &  \\ \cline{2-12}
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                               &                             &                                       &                             &                                       &                             &                                       &                             &                                       &                             &                                       &  &  &  & 
\caption{Experimental setup 2 on the mice protein expression dataset.}
			\end{tabular}
\end{table*}
   #+end_export



   # TODO mention many skipped tests for this dataset

   # TODO detail tableThe 6 features we use are detailed below.

   # !!! TODO create variable table for above

   # TODO With or without DAG independence encoding
 |-------------+---------------------------------------+---------+--------|
 | /           | <                                     |       < |      < |
 | Model       | Algorithm                             |     BIC | Stages |
 |-------------+---------------------------------------+---------+--------|
 | CStree      | Modified Algorithm \ref{alg:cstreepc} | -5433.0 |    133 |
 | Staged tree | Hill climbing                         | -5320.1 |     28 |
 | Staged tree | Backward hill climbing                | -5227.6 |     36 |
 | Staged tree | Backward joining                      | -6427.1 |    148 |
 | Staged tree | Hierarchial clustering                | -5990.4|     17 |
   # TODO Print all possible orderings

   # TODO mention that encoding CI relations before hand helps with data sparsity
   # TODO run coronary dataset again to consider ALL orderings

 {{{NEWPAGE}}}


** Supersymmetry data

   This is a simulated dataset involving 8 kinematic features measured by detectors in a particle accelerator alongside 10 more high-level features which are functions of the original 8 features cite:baldi-2014-searc-exotic. The aim is is to identify the class label for each sample which denotes whether the measurement corresponds to a signal or background event. The dataset contains in total 5000000 samples. There are no missing values in this dataset, and we apply it on the 8 low-level features, and after taking a random subsample of size 2500000 samples. Our aim with using this dataset is to assess the empirical feasibility of our methods on large sample sizes. The number of valid causal orderings varies largely depending on the subsampled data, which ranged from 200 to 8000 in our experiments. As a result, we simply learn the CStree with the first ordering in the algorithm. 

        #+begin_export latex
\begin{figure}[H]\label{fig:susy1}
   \begin{floatrow}
\ffigbox{\includegraphics[width=1.1\linewidth]{temp/susy1_cstree.pdf}}%
\caption{First CStree learnt from the supersymmetry dataset.}
        
   \end{floatrow}
\end{figure}
   #+end_export

   It is expected that since the supersymmetry data involves many causal interactions, there will be few independence relations, which is what we see from the CStree.

   # TODO With DAG, without DAG, number of samples changes
                                        

** Vitamin-D dataset
   This dataset comes from a real study on Vitamin D and mortality cite:martinussen-2017-instr-variab, and contains 4 feature variables corresponding to age, a binary indicator denoting mutations in the filaggrin gene, vitamin D levels measured  as serum 25-OH-D (nmol/L), follow up time, alongside the predictor variable which is a binary value indicating knowledge of whether the subject passed away during the follow up. Looking at the data it is clear that the ages are grouped into 4 distinct groups. We group the vitamin D measurement into 4 groups based on the quartiles of the data, and the follow up time is grouped into a binary outcome with the median being the cutoff point. The resulting dataset is summarized in the table below.

 |----------------+------+---------------------------------------------------------------------|
 | /              |    < | <                                                                   |
 | Variable name  | Node | Outcomes                                                            |
 |----------------+------+---------------------------------------------------------------------|
 | Age            |    1 | $\{[40,47),[47,57),[57,67),[67,80)\}$                               |
 | Filaggrin      |    2 | $\{Yes,No\}$                                                        |
 | Vitamin D      |    3 | $\{ [-\infty,Q^3_1),[Q^3_1,Q^3_2),[Q^3_2,Q^3_3),[Q^3_1,-\infty) \}$ |
 | Follow up time |    4 | $\{(-\infty, Q^4_2],(Q^4_2, \infty) \}$                             |
 | Passed away    |    5 | $\{Yes,No \}$                                                       |

Here $Q^i_1,Q^i_2,Q^i_3$ respectively denote the lower quartile, median and upper quartile of the variable corresponding to node $i$.


   We perform the experimental setup 1 and 2 on this dataset and get the following results.

   #+begin_export latex
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[]\label{table:vitd1}
\begin{tabular}{cccccccccccccccc}
\cline{3-12}
                        & \multicolumn{1}{c|}{\multirow{\begin{tabular}[c]{@{}c@{}}Goal:\\ Min stages\end{tabular}}} & \multicolumn{2}{c|}{Anderson}                                           & \multicolumn{2}{c|}{Epps}                                           & \multicolumn{2}{c|}{SKL $5\times 10^{-1}$}                                         & \multicolumn{2}{c|}{SKL $5	\times 10^{-2}$}                                     & \multicolumn{2}{c|}{SKL $5\times 10^{-4}$}                                     &  &  &  &  \\ \cline{3-12}
                        & \multicolumn{1}{c|}{}                                                                            & \multicolumn{1}{c|}{Stages}     & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages}     & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              &  &  &  &  \\ \cline{3-12}
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{12}         & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{12}         & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{PC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5}          & \multicolumn{1}{c|}{13}     & \multicolumn{1}{c|}{-8800.0}          & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5}          & \multicolumn{1}{c|}{5}      & \multicolumn{1}{c|}{-8771.7}          & \multicolumn{1}{c|}{9}      & \multicolumn{1}{c|}{-8532.3}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{6}          & \multicolumn{1}{c|}{-8480.1} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1}          & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5}          & \multicolumn{1}{c|}{9}      & \multicolumn{1}{c|}{\textbf{-8405.2}}          & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             &                                       &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                         & \multicolumn{1}{c|}{26}         & \multicolumn{1}{c|}{-8462.3}          & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-8462.3}          & \multicolumn{1}{c|}{26}         & \multicolumn{1}{c|}{-8462.3}          & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-8462.3}          & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-8462.3}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{HC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                     & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5}          & \multicolumn{1}{c|}{36}     & \multicolumn{1}{c|}{-8871.7}          & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5}          & \multicolumn{1}{c|}{5}      & \multicolumn{1}{c|}{-8771.7}          & \multicolumn{1}{c|}{9}      & \multicolumn{1}{c|}{-8526.3}          &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                   & \multicolumn{1}{c|}{5}          & \multicolumn{1}{c|}{-8771.7}          & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-8701.7}          & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5}          & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-8413.0} & \multicolumn{1}{c|}{14}     & \multicolumn{1}{c|}{-8418.0}          &  &  &  &  \\ \cline{2-12}
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             & \textbf{}                             &  &  &  &  \\
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             &                                       &  &  &  &  \\
                        &                                                                                                  &                                 &                                       &                             &                                       &                                 &                                       &                             &                                       &                             &                                       &  &  &  & 
\caption{Experimental setup 1 on the Vitamin D dataset}
			\end{tabular}
\end{table*}





% Please add the following required packages to your document preamble:
% \usepackage{multirow}
\begin{table*}[]
\begin{tabular}{cccccccccccccccc}
\cline{3-12}
                        & \multicolumn{1}{c|}{\multirow{\begin{tabular}[c]{@{}c@{}}Goal:\\ Max BIC\end{tabular}}} & \multicolumn{2}{c|}{Anderson}                              & \multicolumn{2}{c|}{Epps}                                  & \multicolumn{2}{c|}{SKL $5\times10^{-1}$}                                & \multicolumn{2}{c|}{SKL $5	\times10^{-2}$}                                     & \multicolumn{2}{c|}{SKL $5\times 10^{-4}$}                            &  &  &  &  \\ \cline{3-12}
                        & \multicolumn{1}{c|}{}                                                                         & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages}     & \multicolumn{1}{c|}{BIC}     & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}              & \multicolumn{1}{c|}{Stages} & \multicolumn{1}{c|}{BIC}     &  &  &  &  \\ \cline{3-12}
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             &                              &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                      & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{12}         & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1}          & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{PC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                  & \multicolumn{1}{c|}{6}      & \multicolumn{1}{c|}{-8487.9} & \multicolumn{1}{c|}{14}     & \multicolumn{1}{c|}{-8500.0} & \multicolumn{1}{c|}{5}          & \multicolumn{1}{c|}{-8749.9} & \multicolumn{1}{c|}{19}     & \multicolumn{1}{c|}{-8428.2}          & \multicolumn{1}{c|}{37}     & \multicolumn{1}{c|}{-8489.0} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                & \multicolumn{1}{c|}{6}      & \multicolumn{1}{c|}{-8480.1} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} & \multicolumn{1}{c|}{5}          & \multicolumn{1}{c|}{-8749.9} & \multicolumn{1}{c|}{9}      & \multicolumn{1}{c|}{\textbf{-8405.2}} & \multicolumn{1}{c|}{12}     & \multicolumn{1}{c|}{-8410.1} &  &  &  &  \\ \cline{2-12}
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             &                              &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{DAG}                                                                      & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-8462.3} & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-8462.3} & \multicolumn{1}{c|}{26}         & \multicolumn{1}{c|}{-8462.3} & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-8462.3}          & \multicolumn{1}{c|}{26}     & \multicolumn{1}{c|}{-8462.3} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{\rotatebox{90}{HC}} & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w DAG\end{tabular}}                  & \multicolumn{1}{c|}{5}      & \multicolumn{1}{c|}{-8779.6} & \multicolumn{1}{c|}{36}     & \multicolumn{1}{c|}{-8871.7} & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5} & \multicolumn{1}{c|}{8}      & \multicolumn{1}{c|}{-8487.9}          & \multicolumn{1}{c|}{13}     & \multicolumn{1}{c|}{-8525.8} &  &  &  &  \\ \cline{2-12}
\multicolumn{1}{c|}{}   & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}CSTree \\ w/o DAG\end{tabular}}                & \multicolumn{1}{c|}{5}      & \multicolumn{1}{c|}{-8771.7} & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-8701.7} & \multicolumn{1}{c|}{\textbf{4}} & \multicolumn{1}{c|}{-9041.5} & \multicolumn{1}{c|}{11}     & \multicolumn{1}{c|}{-8413.0}          & \multicolumn{1}{c|}{14}     & \multicolumn{1}{c|}{-8418.0} &  &  &  &  \\ \cline{2-12}
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             &                              &  &  &  &  \\
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             &                              &  &  &  &  \\
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             &                              &  &  &  &  \\
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             & \textbf{}                    &  &  &  &  \\
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             &                              &  &  &  &  \\
                        &                                                                                               &                             &                              &                             &                              &                                 &                              &                             &                                       &                             &                              &  &  &  & 

\caption{Experimental setup 2 on the Vitamin D dataset}
			\end{tabular}
\end{table*}
   #+end_export

   # TODO Analysis of vitamind 1 exp 1
   #+CAPTION: Ordering 53421
  |-------------+---------------------------------------+---------+--------|
  | /           | <                                     |       < |      < |
  | Model       | Algorithm                             |     BIC | Stages |
  |-------------+---------------------------------------+---------+--------|
  | CStree      | Modified Algorithm \ref{alg:cstreepc} | -8405.2 |      9 |
  | Staged tree | Hill climbing                         | -8423.1 |     11 |
  | Staged tree | Backward hill climbing                | -8423.4 |     12 |
  | Staged tree | Backward joining                      | -8560.8 |     26 |
  | Staged tree | Hierarchial clustering                | -8910.0 |     10 |


   # !!! TODO Cite table number above
   #+CAPTION: Ordering 32145
  |-------------+---------------------------------------+---------+--------|
  | /           | <                                     |       < |      < |
  | Model       | Algorithm                             |     BIC | Stages |
  |-------------+---------------------------------------+---------+--------|
  | CStree      | Modified Algorithm \ref{alg:cstreepc} | -8776.5 |      9 |
  | Staged tree | Hill climbing                         | -8406.5 |     13 |
  | Staged tree | Backward hill climbing                | -8403.5 |     16 |
  | Staged tree | Backward joining                      | -8547.1 |     19 |
  | Staged tree | Hierarchial clustering                | -8628.6 |     10 |

Which is why we fix the last 2 variables TODO Explain this part
Technically, the highest BIC score was achieved using the Epps test, which gives the CStree with ordering 12345 a BIC score of -8734.5 however this CStree has 48 stages. This is followed by the CStree obtained using the symmetric KL divergence with a threshold of $5 \times 10^{-2}$, which gives the ordering 21345 with a BIC score of -8771.7. However, this is a DAG model with one edge $4 \rightarrow 5$ in the empty context graph. We found changing this threshold to $5 \times 10^{-3}$ gave us a slightly worse BIC score but a CStree that encodes interesting context specific information, which is why we choose this ordering as a basis for comparison with staged trees. We show this CStree alongside its minimal context DAGs below.


   
   # TODO Print all possible orderings

   # TODO Skipped tests due to single values


   
** Discussion of empirical performance

   We see in general that using the symmetric KL divergence to identify stages in the CStree result a higher BIC score when compared to choosing a CStree with the minimum number of stages. There are many reasons which could lead to this happening in practice, with the first being unreliability of the statistical independence tests, which is in general a very hard task cite:shah-2020-hardn-condit.


   If the data generating distribution happens to be faithful to a CStree with a large number of stages, particularly non-singleton stages, then it is possible that there is not enough samples to learn the context specific information in the finite data setting. We note that a lot of this information was learnt by comparing samples with extremely different sizes, so in practice it might be a good idea to merge the corresponding nodes into the same stage if a certain criteria is met, for example, only if both sample sizes are at least half of their mean.


   The instability of DAG learning algorithms used to get the causal orderings also play a crucial role, and we observed this in practice. For example, we observed that if we happen to know the causal ordering and then feed it to the algorithm, we could learn a DAG in the initial step whose MEC contains no DAGs which are consistent with this causal ordering.  This may also present itself to be a problem if we have partial knowledge of the ordering, for example in the mice cortex data experiment, it is plausible to think of the predictor class to be a function of the gene expression levels.


   One important aspect of this process is to choose the best CStree from all possible causal ordering, which is a model selection problem. In Algorithm \ref{alg:cstreepc} we opt to choose the one with the fewest stages since it relates to choosing the model with the fewest parameters, i.e. the simplest model. This goes in hand with the principle of Occam's razor - in the face of many possible models, choose the simplest model. Here a simple model refers to one which has few parameters. In practice however we see that problems arising due to misleading conditional independence tests and small data samples might lead to the simplest model being too simple, compromising the fit to the data.


   # TODO Something about orderings


* Conclusions
** Summary
   We start with DAGs as a means to encode CI relations, and how one can use the characterization of Markov Equivalence in DAGs to learn causal structure through CI testing. We then cover the limitations of CI relations in comparison to CSI relations, and go over CStrees as a means to encode such CSI relations. We then show how one can learn these CSI relations from observational data and learn a CStree, and how to compute minimal contexts which are important when it comes to visualizing higher dimensional CStrees. We then apply these techniques to synthetic and real data.

   
** Future work
   One of the more natural extensions of this work is to learn CStrees from interventional data. This can already be done with DAGs cite:yang-2018-charac-learn. On the topic of model selection, it might be interesting to see the applicability of Bayesian model selection for CStrees, whereby the model evidence is used as the basis for comparing models which automatically penalizes over-complex models whilst also penalizing models which do not agree with the observed data cite:mackay-1992-bayes-inter. Similar approaches have been applied to Gaussian DAG models cite:castelletti-2020-bayes-model. Generalization to missing data problems would also be an interesting avenue, considering that the PC algorithm has been recently extended for missing data instances in DAGs cite:tu-2019-causal-discov. But perhaps the most impactful extension would be formulating the problem of finding the best CStree into a continuous optimization problem, which can lead to scalable score based methods. Recent work has formulate the problem of searching for the best DAG according to some metric by using a characterization of acyclicity that is smooth and exact, allowing the conversion of the combinatorial problem into a purely continuous problem cite:zheng-2018-dags-no-tears. 
   

   

{{{NEWPAGE}}}



bibliographystyle:unsrt
bibliography:~/Dropbox/org/bibliography/references.bib

{{{NEWPAGE}}}
# * TODO General
#  - DAG To CStree, (1,4)(1,3)(3,5) 2 alone did not work?
#    Also (1,2),(1,4),(1,5),(2,4),(2,5)
#  - Recheck gettign CSI relations from the tree

#  - On recovering the empty context graph but with the wrong trees

#  - Explicitly mention getting the case where for minimal contexts you can have for a given pair contexts involving (1,2,3), (1,2) and (4,5) in which case T must be empty
#    A very important case that the contexts provided 

 # - Explain the special case for the DAG to cstree algorithm, second variable if independent of the first this means 2_||_1 and we colour all nodes in level 1 the same as well
 # - put paths and blocked paths visualization, intuition of information flowing

# faithfulness as a particular form of occams razor

# experimentation - randomized controlledtrials , A/b testings, visualize like mooijs slide separating sample into subsets, applying intervention and plcebo, measureing outcomes etc
# - pipeline visualization
# - rcts as dags, assumptions  - outcome does not cause treatment, outcome and treatment are unconfounded cite mooij paper joint causal inference a unifyin g perspective

# - table with number of dags and cstrees tabel
# - increasing sequence of sets from distributions to markov distributions od dags, to markov distributions of cstrees

# - separate the use of DAGs for probabilistic modelling, and causal modelling - fundamental assumption of different semantics where the observational and interventiona ldistributions are the same for no parents. then no reason to assume parent-child distributions should stay the same under intervention - modularity property.
# - probabilistic dag and causal dag are different! 
# - extended conditional independence when ci relations contain non random variables
# - example of d separation in dags, student studying dag example and use this for maybe the cstree sampling example
# - bayes ball
#  - causal ordering is for trees, linear extension topological ordering is for dags
# - cycles and confounders with mooijs paper in dag limitations
#  - Think about storing stage information in the node itself
#  - http://swoh.web.engr.illinois.edu/courses/IE598/handout/markov.pdf Read intersection lemma and relate to compositional models, relate to pairwise minimal contexts, also read on graphoid case equivalence http://www.stats.ox.ac.uk/~steffen/teaching/cimpa/markov.pdf
#  - For early test stopping make sure CStree property is maintained
#  - Small optimizations e.g. resevouir sampling for randomized node testing
#  - Make efficient use of the Tufte eco system: marginnote, sidenote{<number>}{<offset>}{text}
#  - Desibe the 3 basic DAGs in english for example colliders making two causes compete for each other
#  - Traffic dataset
#  - Generate the data with the aim of making atleast one row all white after DAG to Cstree conversion AND making sure it can learn the DAG from PC first
#  - Gene data in http://proceedings.mlr.press/v119/saeed20a/saeed20a.pdf
#  - PC Python from pgmpy, PC psuedocode from Neapolitan
#  - Quantum paper https://iopscience.iop.org/article/10.1088/1367-2630/17/3/033002/pdf
#  - Marloes Maathuis, High dimensional consistency in score-based and hybrid structure learning: A DAG G is a perfect map of a distribution P if the d-separations in G = conditional independencies in P
#  - Number of DAGs with p nodes and number of CStrees with p binary valued variables
#  - ETH Seminar notes: Constructing minimal I-MAP by taking an ordering, writing the full factorization, drawing corresponding DAG. I-MAPs are important because every distribution is Markov wtih respect to a full DAG, and equivalently, a full DAG is an I-MAP of any distribution. Causal sufficiency i.e. no hidden confounders.
# - add special case for fully connected dag when converting to cstree
 #   - Things added after first draft to Liam: Perfect maps, mention of having to pick cstree based on some criteria
 # - average of number of stages from random dags w varying p
 # average size of pairwise graphoid

  #  - Using algorithmic2e to write functions instead of procedural code https://tex.stackexchange.com/questions/280008/algorithm2e-writing-simple-pseudocode-with-multiple-functions/280038

  # - some cases we might know part of the dag structure for sure in this case maybe its good to add an experiment for this

# Things about model selection
# Intervention data http://www.sciencemag.org/content/suppl/2005/04/21/308.5721.523.DC1/Sachs.SOM.Datasets.zip

* Appendix A
** Remarks on implementation
   Implementation of this work was done in Python. Draft code repository is available at github.com/mnazaal/masters-thesis. This document was generated with org-mode. Use of scikit, pandas, numpy. Test driven development with pytest.
